<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet ekr_test?>
<leo_file>
<leo_header file_format="2" tnodes="0" max_tnode_index="0" clone_windows="0"/>
<globals body_outline_ratio="0.5" body_secondary_ratio="0.5">
	<global_window_position top="50" left="50" height="500" width="700"/>
	<global_log_window_position top="0" left="0" height="0" width="0"/>
</globals>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="newlife.20101028111308.1240"><vh>@chapters</vh></v>
<v t="newlife.20101028111308.1244"><vh>memchached</vh>
<v t="newlife.20101028111308.1239"><vh>What it Does</vh></v>
<v t="newlife.20101028111308.1241"><vh>What Memcached Is</vh></v>
<v t="newlife.20101028111308.1242"><vh>An Adventure in Learning Memcached</vh></v>
</v>
<v t="newlife.20101028111308.1245"><vh>python-memcached</vh>
<v t="newlife.20101101104322.1261" a="O"><vh>@file /home/newlife/下载/python-memcached-1.45/memcache.py</vh>
<v t="newlife.20101101104322.1262"><vh>memcache declarations</vh></v>
<v t="newlife.20101101104322.1263"><vh>cmemcache_hash</vh></v>
<v t="newlife.20101101104322.1264"><vh>useOldServerHashFunction</vh></v>
<v t="newlife.20101101104322.1265"><vh>class _Error</vh></v>
<v t="newlife.20101101104322.1266"><vh>class Client</vh>
<v t="newlife.20101101104322.1267"><vh>class MemcachedKeyError</vh></v>
<v t="newlife.20101101104322.1268"><vh>class MemcachedKeyLengthError</vh></v>
<v t="newlife.20101101104322.1269"><vh>class MemcachedKeyCharacterError</vh></v>
<v t="newlife.20101101104322.1270"><vh>class MemcachedKeyNoneError</vh></v>
<v t="newlife.20101101104322.1271"><vh>class MemcachedKeyTypeError</vh></v>
<v t="newlife.20101101104322.1272"><vh>class MemcachedStringEncodingError</vh></v>
<v t="newlife.20101101104322.1273"><vh>__init__</vh></v>
<v t="newlife.20101101104322.1274"><vh>set_servers</vh></v>
<v t="newlife.20101101104322.1275"><vh>get_stats</vh></v>
<v t="newlife.20101101104322.1276"><vh>get_slabs</vh></v>
<v t="newlife.20101101104322.1277"><vh>flush_all</vh></v>
<v t="newlife.20101101104322.1278"><vh>debuglog</vh></v>
<v t="newlife.20101101104322.1279"><vh>_statlog</vh></v>
<v t="newlife.20101101104322.1280"><vh>forget_dead_hosts</vh></v>
<v t="newlife.20101101104322.1281"><vh>_init_buckets</vh></v>
<v t="newlife.20101101104322.1282"><vh>_get_server</vh></v>
<v t="newlife.20101101104322.1283"><vh>disconnect_all</vh></v>
<v t="newlife.20101101104322.1284"><vh>delete_multi</vh></v>
<v t="newlife.20101101104322.1285"><vh>delete</vh></v>
<v t="newlife.20101101104322.1286"><vh>incr</vh></v>
<v t="newlife.20101101104322.1287"><vh>decr</vh></v>
<v t="newlife.20101101104322.1288"><vh>_incrdecr</vh></v>
<v t="newlife.20101101104322.1289"><vh>add</vh></v>
<v t="newlife.20101101104322.1290"><vh>append</vh></v>
<v t="newlife.20101101104322.1291"><vh>prepend</vh></v>
<v t="newlife.20101101104322.1292"><vh>replace</vh></v>
<v t="newlife.20101101104322.1293"><vh>set</vh></v>
<v t="newlife.20101101104322.1294"><vh>cas</vh></v>
<v t="newlife.20101101104322.1295"><vh>_map_and_prefix_keys</vh></v>
<v t="newlife.20101101104322.1296"><vh>set_multi</vh></v>
<v t="newlife.20101101104322.1297"><vh>_val_to_store_info</vh></v>
<v t="newlife.20101101104322.1298"><vh>_set</vh></v>
<v t="newlife.20101101104322.1299"><vh>_get</vh></v>
<v t="newlife.20101101104322.1300"><vh>get</vh></v>
<v t="newlife.20101101104322.1301"><vh>gets</vh></v>
<v t="newlife.20101101104322.1302"><vh>get_multi</vh></v>
<v t="newlife.20101101104322.1303"><vh>_expect_cas_value</vh></v>
<v t="newlife.20101101104322.1304"><vh>_expectvalue</vh></v>
<v t="newlife.20101101104322.1305"><vh>_recv_value</vh></v>
<v t="newlife.20101101104322.1306"><vh>check_key</vh></v>
</v>
<v t="newlife.20101101104322.1307"><vh>class _Host</vh>
<v t="newlife.20101101104322.1308"><vh>__init__</vh></v>
<v t="newlife.20101101104322.1309"><vh>debuglog</vh></v>
<v t="newlife.20101101104322.1310"><vh>_check_dead</vh></v>
<v t="newlife.20101101104322.1311"><vh>connect</vh></v>
<v t="newlife.20101101104322.1312"><vh>mark_dead</vh></v>
<v t="newlife.20101101104322.1313"><vh>_get_socket</vh></v>
<v t="newlife.20101101104322.1314"><vh>close_socket</vh></v>
<v t="newlife.20101101104322.1315"><vh>send_cmd</vh></v>
<v t="newlife.20101101104322.1316"><vh>send_cmds</vh></v>
<v t="newlife.20101101104322.1317"><vh>readline</vh></v>
<v t="newlife.20101101104322.1318"><vh>expect</vh></v>
<v t="newlife.20101101104322.1319"><vh>recv</vh></v>
<v t="newlife.20101101104322.1320"><vh>__str__</vh></v>
</v>
<v t="newlife.20101101104322.1321"><vh>_doctest</vh></v>
</v>
<v t="newlife.20101028111308.1243"><vh>python-memcached的一个疑问</vh></v>
<v t="newlife.20101101104322.1322"><vh>流程分析</vh></v>
</v>
<v t="newlife.20101028111308.1246" a="EM"><vh>django.cache</vh>
<v t="newlife.20101028111308.1247"><vh>core.cache.__init__.py</vh></v>
<v t="newlife.20101028111308.1248"><vh>code.cache.backeds.memcached.py</vh></v>
<v t="newlife.20101110110210.1404"><vh>views.decorator.cache</vh></v>
<v t="newlife.20101110110210.1405"><vh>middleware.cache</vh></v>
<v t="newlife.20101110110210.1406"><vh>decorator</vh></v>
<v t="newlife.20101110110210.1407"><vh>关于django的中间件</vh></v>
</v>
<v t="newlife.20101109103546.1392"><vh>misc</vh>
<v t="newlife.20101101104322.1323"><vh>cache framework</vh></v>
<v t="newlife.20101108160359.1387"><vh>huanju:/contrib/cache.py</vh></v>
<v t="newlife.20101108160359.1388"><vh>夏清然的blog</vh></v>
<v t="newlife.20101109103546.1391"><vh>想法</vh></v>
</v>
<v t="newlife.20101109103546.1393"><vh>keyedcache</vh>
<v t="newlife.20101109103546.1394"><vh>source code</vh></v>
<v t="newlife.20101109103546.1395"><vh>记录</vh></v>
<v t="newlife.20101109103546.1396"><vh>遍历memcache</vh></v>
</v>
<v t="newlife.20101110110210.1403"><vh>写啥阿。。</vh>
<v t="newlife.20101110110210.1408"><vh>写这个</vh></v>
<v t="newlife.20101112095312.1415"><vh>有参数的decorator</vh></v>
<v t="newlife.20101112095312.1416"><vh>优雅的写法。</vh></v>
<v t="newlife.20101117145351.1419"><vh>wraps</vh>
<v t="newlife.20101117145351.1420"><vh>What does functools.wraps do?</vh></v>
<v t="newlife.20101117145351.1421"><vh>another wraps</vh></v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="newlife.20101028111308.1239">memcached allows you to take memory from parts of your system where you have more than you need and make it accessible to areas where you have less than you need.

memcached允许你从用你的的系统种多出的内存，并且使这些内存放到需要的地方。

memcached also allows you to make better use of your memory. If you consider the diagram to the right, you can see two deployment scenarios:
memcached还可以是你更加充分的利用你的内存。如果你仔细分析右边的图，你会发现这样2个开发场景。    
##http://memcached.org/memcached-usage.png，，图在这里。
Each node is completely independent (top).
每个节点都是独立的。
Each node can make use of memory from other nodes (bottom).
每个节点都可以利用别的节点的内存。

The first scenario illustrates the classic deployment strategy, however you'll find that it's both wasteful in the sense that the total cache size is a fraction of the actual capacity of your web farm, but also in the amount of effort required to keep the cache consistent across all of those nodes.
第一个场景显示了正常的部署策略，可是你会发现，可用的缓存只是实际缓存的一部分，而且还要做更多的努力来保持这些节点的数据的一致性。这是多么的浪费阿

With memcached, you can see that all of the servers are looking into the same virtual pool of memory. This means that a given item is always stored and always retrieved from the same location in your entire web cluster.
第二个场景就是有memcached，我们可以看到，所有的服务区都去同一个虚拟的缓存池里去找数据，这就意味这某一个对象一直都是在你整个web集群的同一个位置读取的，

Also, as the demand for your application grows to the point where you need to have more servers, it generally also grows in terms of the data that must be regularly accessed. A deployment strategy where these two aspects of your system scale together just makes sense.

另外，随着系统应用的增长，我们需要更多的服务器，我们需要缓存的数据也会越来越多，这个时候部署策略就有意义了。

The illustration to the right only shows two web servers for simplicity, but the property remains the same as the number increases. If you had fifty web servers, you'd still have a usable cache size of 64MB in the first example, but in the second, you'd have 3.2GB of usable cache.

该图为了简化，只是用了2个web 服务器，但是随着服务器数量的增长，意义是一样的。如果你有50个web服务器，在第一种情况的时候你仍然有不可用的64m缓存，后一种就有3.2G的可用的缓存。

Of course, you aren't required to use your web server's memory for cache. Many memcached users have dedicated machines that are built to only be memcached servers.
当然，没有要求你的web服务器的内存来作缓存，许多memcached的用户有单独的及其仅用作缓存服务器。</t>
<t tx="newlife.20101028111308.1240">What it Does


memcached allows you to take memory from parts of your system where you have more than you need and make it accessible to areas where you have less than you need.

memcached also allows you to make better use of your memory. If you consider the diagram to the right, you can see two deployment scenarios:

Each node is completely independent (top).
Each node can make use of memory from other nodes (bottom).
The first scenario illustrates the classic deployment strategy, however you'll find that it's both wasteful in the sense that the total cache size is a fraction of the actual capacity of your web farm, but also in the amount of effort required to keep the cache consistent across all of those nodes.

With memcached, you can see that all of the servers are looking into the same virtual pool of memory. This means that a given item is always stored and always retrieved from the same location in your entire web cluster.

Also, as the demand for your application grows to the point where you need to have more servers, it generally also grows in terms of the data that must be regularly accessed. A deployment strategy where these two aspects of your system scale together just makes sense.

The illustration to the right only shows two web servers for simplicity, but the property remains the same as the number increases. If you had fifty web servers, you'd still have a usable cache size of 64MB in the first example, but in the second, you'd have 3.2GB of usable cache.

Of course, you aren't required to use your web server's memory for cache. Many memcached users have dedicated machines that are built to only be memcached servers.</t>
<t tx="newlife.20101028111308.1241">Memcached

Free &amp; open source, high-performance, distributed memory object caching system, generic in nature, but intended for use in speeding up dynamic web applications by alleviating database load.
自由，开源，高效的分布式对象缓存系统，原生动态，致力于加速动态的web应用来减少数据库负载。


Memcached is an in-memory key-value store for small chunks of arbitrary data (strings, objects) from results of database calls, API calls, or page rendering.

Memcached 是一个在内存种存储人工对象的key-value，存储的对象可以是数据库返回的查询结果，API调用，页面渲染。

Memcached is simple yet powerful. Its simple design promotes quick deployment, ease of development, and solves many problems facing large data caches. Its API is available for most popular languages.

简单但是高效，设计用来迅速部署，易于开发，解决大量数据缓存的问题。对大多数流行语言都有相应的api

At heart it is a simple Key/Value store.
核心就是一个key/value的存储。

Memcached In Slightly More Words
稍复杂的解释，
See the memcached.org about page for a brief overview.

I Turned It On and My App Isnot Faster!!!
我开了Memcached，可是我的应用没有变快？？

Memcached is a developer tool, not a "code accelerator", nor is it database middleware. If you're trying to set up an application you have downloaded or purchased to use memcached, your best bet is to head back their way and read your app's documentation on how to utilize memcached. Odds are these documents will not help you much.

Memcached是一个开发工具，不是代码加速器，也不是数据库中间层，如果你想安装一个你下载或者购买的应用来使用Memcached，你最好是看应用的文档，来知道如何使用Memcached，这样这些文档不会帮助你什么。

What is it Made Up Of?
Memcached由什么组成？

Client software, which is given a list of available memcached servers.
A client-based hashing algorithm, which chooses a server based on the "key" input.
Server software, which stores your values with their keys into an internal hash table.
Server algorithms, which determine when to throw out old data (if out of memory), or reuse memory.
客户端软件，一个可用的缓存服务器的列表，
一个基于客户端的hash算法，用来根据输入的“key”来选择去哪个机器上找.
服务端软件，保存key/value在一个hash表里
服务端算法，用来决定啥时候抛弃旧的数据，或者重内存，

What are the Design Philosophies?
设计理念是啥？/

Simple Key/Value Store
The server does not care what your data looks like. Items are made up of a key, an expiration time, optional flags, and raw data. It does not understand data structures; you must upload data that is pre-serialized. 
Some commands (incr/decr) may operate on the underlying data, but the implementation is simplistic.
简单的key/value存储，服务器不关心你的数据看起来怎么样。对象是有一个key，失效时间，可选的flag，和真实数据。他不理解数据结构，你必须上传提前序列化的数据，
一些命令或许会操作底层数据，但是实现是非常简单的。

Smarts Half in Client, Half in Server
灵活的一半在客户端，一半在服务器端。
A "memcached implementation" is implemented partially in a client, and partially in a server. Clients understand how to send items to particular servers, what
 to do when it cannot contact a server, and how to fetch keys from the servers.
Memcached的实现部分在客户端，部分是服务器端，客户端知道如何发送对象，去具体的服务器，当无法连接服务器如何处理，如何从服务器端取得keys 

The servers understand how to receive items, and how to expire them.
服务器知道如何接收对象，如何释放。

Servers are Disconnected From Each Other
服务器之间不相互通信
Memcached servers are generally unaware of each other. There is no crosstalk, no syncronization, no broadcasting. The lack of interconnections means adding more servers will usually add more capacity as you expect. There might be exceptions to this rule, but they are exceptions and carefully regarded.
Memcached服务器之间不知道彼此的存在，没有沟通，没有同步，没有广播。没有交互就意味这增加更多的服务器只会增加你所预期的空间，这或许有意外，但是被仔细考虑过了。

O(1) Everything
所有的复杂度都是O(1)
For everything it can, memcached commands are O(1). Each command takes roughly the same amount of time to process every time, and should not get noticably slower anywhere. This goes back to the "Simple K/V Store" principle, as you do not want to be processing data in the cache service your webservers may need to access at the same time.
对于Memcached，他所做的所有的事情的复杂度都是0(1)，每个指令每次需要花费相同的时间来处理，在任何地方都不应该有明显的缓慢。回到简单的K/V存储原则，因为你不希望
#啥意思阿，。，。

Forgetting Data is a Feature
无视数据是一个特色。
Memcached is, by default, a Least Recently Used cache. It is designed to have items expire after a specified amount of time. Both of these are elegant solutions to many problems; Expire items after a minute to limit stale data being returned, or flush unused data in an effort to retain frequently requested information.
Memcached 是一个最近用过数据的缓存。被设计有一些经过特定时间过期的对象，这些都是很多问题的优雅的解决办法，过期时间
#orz

This further allows great simplification in how memcached works. No "pauses" waiting for a garbage collector ensures low latency, and free space is lazily reclaimed.

Cache Invalidation is a Hard Problem
缓存验证是一个严重的问题。
Given memcached ～s centralized-as-a-cluster nature, the job of invalidating a cache entry is trivial. Instead of broadcasting data to all available hosts, clients direct in on the exact location of data to be invalidated. You may further complicate matters to your needs, and there are caveats, but you sit on a strong baseline.

How Is Memcached Evolving?

Memcached has been evolving as a platform. Some of this is due to the slow development nature, and the many clients. Mostly it happens as people explore the possibilities of K/V stores. Learning to cache SQL queries and rendered templates used to keep developers occupied, but they thirst for more.

The Protocol
Memcached is not the only implementation of the protocol. There are commercial entities, other OSS projects, and so on. Memcached clients are fairly common, and there are many uses for a memcached-like cluster layout. We will continue to see other projects "speak" memcached, which in turn influences memcached as a culture and as software itself.

Other Protocols
The industry is experimenting with many different ways to communicate with networked services. Google protocol buffers, Facebook Thrift, Avro, etc. There might be better ways to do this, as the future will show.

Persistent Storage
Many users want K/V stores that are able to persist values beyond a restart, or beyond available physical memory. In many cases memcached is not a great fit here; expensive flash memory is needed to keep data access performant. In most common scenarios, disaster can ensue if a server is unavailable and later comes up with old data. Users see stale data, features break, etc.

However, with the price of SSD~s dropping, this is likely an area to be expanded into.

Storage Engines
Storage Engines in general are an important future to memcached as a service. Aside from our venerable slabbing algorithm, there are other memory backends to experiment with. tcmalloc, jemalloc, CPU-local slabs, hierarchies, etc. Storage engines foster experimentation to improve speed and memory efficiency, as well as specialized services able to speak the memcached protocol.</t>
<t tx="newlife.20101028111308.1242">Two plucky adventurers, Programmer and Sysadmin, set out on a journey. Together they make websites. Websites with webservers and databases. Users from all over the Internet talk to the webservers and ask them to make pages for them. The webservers ask the databases for junk they need to make the pages. Programmer codes, Sysadmin adds webservers and database servers.

2个大胆的冒险家，程序员和系统管理员，开始旅行了。他们一起作一个网站，有数据库web服务器和数据库。用户访问web服务器，请求所要的页面，web服务器请求数据库得到返回页面所需的数据，程序员编程，SA加服务器

One day the Sysadmin realizes that their database is sick! It is spewing bile and red stuff all over! Sysadmin declares it has a fever, a load average of 20! Programmer asks Sysadmin, "well, what can we do?" Sysadmin says, "I heard about this great thing called memcached. It really helped livejournal!" "Okay, let's try it!" says the Programmer.

一天，SA发现数据库病了，然后Memcache出现了。

Our plucky Sysadmin eyes his webservers, of which he has six. He decides to use three of them to run the 'memcached' server. Sysadmin adds a gigabyte of ram to each webserver, and starts up memcached with a limit of 1 gigabyte each. So he has three memcached instances, each can hold up to 1 gigabyte of data. So the Programmer and the Sysadmin step back and behold their glorious memcached!
SA看着6台web服务器，决定用其中的三个作缓存服务器，给这个三个服务器每一加1G的内存，启动Memcache限制1G，这样他就有了3个Memcache实例，每个可以保存1G的数据。然后SA和coder看着3个缓存服务器

"So now what?" they say, "it's not DOING anything!" The memcacheds aren't talking to anything and they certainly don't have any data. And NOW their database has a load of 25!

结果当然是啥页没有，数据库负载到25

Our adventurous Programmer grabs the pecl/memcache client library manual, which the plucky Sysadmin has helpfully installed on all SIX webservers. "Never fear!" he says. "I've got an idea!" He takes the IP addresses and port numbers of the THREE memcacheds and adds them to an array in php.

$MEMCACHE_SERVERS = array(
    "10.1.1.1", //web1
    "10.1.1.2", //web2
    "10.1.1.3", //web3
);
Then he makes an object, which he cleverly calls '$memcache'.

$memcache = new Memcache();
foreach($MEMCACHE_SERVERS as $server){
    $memcache-&gt;addServer ( $server );
}
以上是服务端处理的对象。

Now Programmer thinks. He thinks and thinks and thinks. "I know!" he says. "There's this thing on the front page that runs SELECT * FROM hugetable WHERE timestamp &gt; lastweek ORDER BY timestamp ASC LIMIT 50000; and it takes five seconds!" "Let's put it in memcached," he says. So he wraps his code for the SELECT and uses his $memcache object. His code asks:
缓存了返回数据量较大的数据库查询结果
。
Are the results of this select in memcache? If not, run the query, take the results, and PUT it in memcache! Like so:

$huge_data_for_frong_page = $memcache-&gt;get("huge_data_for_frong_page");
if($huge_data_for_frong_page === false){
    $huge_data_for_frong_page = array();
    $sql = "SELECT * FROM hugetable WHERE timestamp &gt; lastweek ORDER BY timestamp ASC LIMIT 50000";
    $res = mysql_query($sql, $mysql_connection);
    while($rec = mysql_fetch_assoc($res)){
        $huge_data_for_frong_page[] = $rec;
    }
    // cache for 10 minutes
    $memcache-&gt;set("huge_data_for_frong_page", $huge_data_for_frong_page, 600);
}

// use $huge_data_for_frong_page how you please
Programmer pushes code. Sysadmin sweats. BAM! DB load is down to 10! The website is pretty fast now. So now, the Sysadmin puzzles, "What the HELL just happened!?" "I put graphs on my memcacheds! I used cacti, and this is what I see! I see traffic to one memcached, but I made three :(." So, the Sysadmin quickly learns the ascii protocol and telnets to port 11211 on each memcached and asks it:

Hey, 'get huge_data_for_front_page' are you there?

The first memcached does not answer...

The second memcached does not answer...

The third memcached, however, spits back a huge glob of crap into his telnet session! There is the data! Only once memcached has the key that the Programmer cached!
只有一太缓存服务器保存了这个大的查询。

Puzzled, he asks on the mailing list. They all respond in unison, "It's a distributed cache! That's what it does!" But what does that mean? Still confused, and a little scared for his life, the Sysadmin asks the Programmer to cache a few more things. "Let's see what happens. We're curious folk. We can figure this one out," says the Sysadmin.
分布式缓存，就是干这个的，

"Well, there is another query that is not slow, but is run 100 times per second. Maybe that would help," says the Programmer. So he wraps that up like he did before. Sure enough, the server loads drops to 8!

缓存经常需要查询的。
So the Programmer codes more and more things get cached. He uses new techniques. "I found them on the list and the faq! What nice blokes," he says. The DB load drops; 7, 5, 3, 2, 1!

"Okay," says the Sysadmin, "let's try again." Now he looks at the graphs. ALL of the memcacheds are running! All of them are getting requests! This is great! They are all used!

So again, he takes keys that the Programmer uses and looks for them on his memcached servers. 'get this_key' 'get that_key' But each time he does this, he only finds each key on one memcached! Now WHY would you do this, he thinks? And he puzzles all night. That's silly! Don't you want the keys to be on all memcacheds?

"But wait", he thinks "I gave each memcached 1 gigabyte of memory, and that means, in total, I can cache three gigabytes of my database, instead of just ONE! Oh man, this is great," he thinks. "This'll save me a ton of cash. Brad Fitzpatrick, I love your ass!"

缓存的空间是相加的，。

"But hmm, the next problem, and this one's a puzzler, this webserver right here, this one runing memcached it's old, it's sick and needs to be upgraded. But in order to do that I have to take it offline! What will happen to my poor memcache cluster? Eh, let's find out," he says, and he shuts down the box. Now he looks at his graphs. Oh noes, the DB load, it is gone up in stride! The load isn't one, it's now two. Hmm, but still tolerable. All of the other memcacheds are still getting traffic. This is not so bad. Just a few cache misses, and I am almost done with my work. So he turns the machine back on, and puts memcached back to work. After a few minutes, the DB load drops again back down to 1, where it should always be.

如果某一天需要升级机器，直接关了就是了，

The cache restored itself! I get it now. If it is not available it just means a few of my requests get missed. But it's not enough to kill me. That's pretty sweet.

So, the Programmer and Sysadmin continue to build websites. They continue to cache. When they have questions, they ask the mailing list or read the faq again. They watch their graphs. And all live happily ever after.

从此，程序员和系统管理员过上了快乐幸福的生活。

</t>
<t tx="newlife.20101028111308.1243">    def _get_server(self, key):
        if isinstance(key, tuple):
            serverhash, key = key
        else:
            serverhash = serverHashFunction(key)

        for i in range(Client._SERVER_RETRIES):
            server = self.buckets[serverhash % len(self.buckets)]
            if server.connect():
                #print "(using server %s)" % server,
                return server, key

            serverhash = serverHashFunction(str(serverhash) + str(i))
        return None, Non
#关于这个的疑问,如果有很多缓存服务器，但是查找的数据不在该服务器上，则不会返回结
#果，这个显然是不正确的,因为按照memcached的介绍，他是一个分布式的缓存架构，在有
#多个缓存服务器的时候，存储在某个服务器上的数据是不会在其他服务器上保存的，除非其
#有一个相当的查找机制。
这个确实有。。如何清晰明了的回答这个问题
    def set_servers(self, servers):
        """
        Set the pool of servers used by this client.

        @param servers: an array of servers.
        Servers can be passed in two forms:
            1. Strings of the form C{"host:port"}, which implies a default weight of 1.
            2. Tuples of the form C{("host:port", weight)}, where C{weight} is
            an integer weight value.
        """
        self.servers = [_Host(s, self.debug) for s in servers]
        self._init_buckets()
库文件包含2个重要的类，_Host和Client，前者代表一个缓存服务器，Client接收的servers这个参数是一个tuple，
set_servers遍历这个元组，构建_Host
关注上面的函数，调用了下面这个函数：
    def _init_buckets(self):
        self.buckets = []
        for server in self.servers:
            for i in range(server.weight):
                self.buckets.append(server)        
这个函数ms没啥用，因为weight都是1，这个函数最后组建的这个buckets对象和servers这个对象么有啥区别，   </t>
<t tx="newlife.20101028111308.1244">传说中的memcache，一个强大的分布式缓存框架？？说框架ms不是特别对。</t>
<t tx="newlife.20101028111308.1245">这个虽然是django推荐的，但是ms好多人都不待见这个</t>
<t tx="newlife.20101028111308.1246">http://python-libmemcached.googlecode.com/svn/trunk/ python-libmemcached</t>
<t tx="newlife.20101028111308.1247">"""
Caching framework.

This package defines set of cache backends that all conform to a simple API.
In a nutshell, a cache is a set of values -- which can be any object that
may be pickled -- identified by string keys.  For the complete API, see
the abstract BaseCache class in django.core.cache.backends.base.

Client code should not access a cache backend directly; instead it should
either use the "cache" variable made available here, or it should use the
get_cache() function made available here. get_cache() takes a backend URI
(e.g. "memcached://127.0.0.1:11211/") and returns an instance of a backend
cache class.

See docs/cache.txt for information on the public API.
"""

from cgi import parse_qsl
from django.conf import settings
from django.core import signals
from django.core.cache.backends.base import InvalidCacheBackendError
from django.utils import importlib

# Name for use in settings file --&gt; name of module in "backends" directory.
# Any backend scheme that is not in this dictionary is treated as a Python
# import path to a custom backend.
BACKENDS = {
    'memcached': 'memcached',
    'locmem': 'locmem',
    'file': 'filebased',
    'db': 'db',
    'dummy': 'dummy',
}

def parse_backend_uri(backend_uri):
    """
    Converts the "backend_uri" into a cache scheme ('db', 'memcached', etc), a
    host and any extra params that are required for the backend. Returns a
    (scheme, host, params) tuple.
    """
    if backend_uri.find(':') == -1:
        raise InvalidCacheBackendError("Backend URI must start with scheme://")
    scheme, rest = backend_uri.split(':', 1)
    if not rest.startswith('//'):
        raise InvalidCacheBackendError("Backend URI must start with scheme://")

    host = rest[2:]
    qpos = rest.find('?')
    if qpos != -1:
        params = dict(parse_qsl(rest[qpos+1:]))
        host = rest[2:qpos]
    else:
        params = {}
    if host.endswith('/'):
        host = host[:-1]

    return scheme, host, params

def get_cache(backend_uri):
    scheme, host, params = parse_backend_uri(backend_uri)
    if scheme in BACKENDS:
        name = 'django.core.cache.backends.%s' % BACKENDS[scheme]
    else:
        name = scheme
    module = importlib.import_module(name)
    return getattr(module, 'CacheClass')(host, params)

cache = get_cache(settings.CACHE_BACKEND)


# Some caches -- pythont-memcached in particular -- need to do a cleanup at the
# end of a request cycle. If the cache provides a close() method, wire it up
# here.
if hasattr(cache, 'close'):
    signals.request_finished.connect(cache.close)
进过分析验证，django内置的cache模块依然没有对多台缓存服务器进行处理阿。
</t>
<t tx="newlife.20101028111308.1248">"Memcached cache backend"

import time

from django.core.cache.backends.base import BaseCache, InvalidCacheBackendError
from django.utils.encoding import smart_unicode, smart_str

try:
    import cmemcache as memcache
    import warnings
    warnings.warn(
        "Support for the 'cmemcache' library has been deprecated. Please use python-memcached instead.",
        PendingDeprecationWarning
    )
except ImportError:
    try:
        import memcache
    except:
        raise InvalidCacheBackendError("Memcached cache backend requires either the 'memcache' or 'cmemcache' library")

class CacheClass(BaseCache):
    def __init__(self, server, params):
        BaseCache.__init__(self, params)
        self._cache = memcache.Client(server.split(';'))

    def _get_memcache_timeout(self, timeout):
        """
        Memcached deals with long (&gt; 30 days) timeouts in a special
        way. Call this function to obtain a safe value for your timeout.
        """
        timeout = timeout or self.default_timeout
        if timeout &gt; 2592000: # 60*60*24*30, 30 days
            # See http://code.google.com/p/memcached/wiki/FAQ
            # "You can set expire times up to 30 days in the future. After that
            # memcached interprets it as a date, and will expire the item after
            # said date. This is a simple (but obscure) mechanic."
            #
            # This means that we have to switch to absolute timestamps.
            timeout += int(time.time())
        return timeout

    def add(self, key, value, timeout=0):
        if isinstance(value, unicode):
            value = value.encode('utf-8')
        return self._cache.add(smart_str(key), value, self._get_memcache_timeout(timeout))

    def get(self, key, default=None):
        val = self._cache.get(smart_str(key))
        if val is None:
            return default
        return val

    def set(self, key, value, timeout=0):
        self._cache.set(smart_str(key), value, self._get_memcache_timeout(timeout))

    def delete(self, key):
        self._cache.delete(smart_str(key))

    def get_many(self, keys):
        return self._cache.get_multi(map(smart_str,keys))

    def close(self, **kwargs):
        self._cache.disconnect_all()

    def incr(self, key, delta=1):
        try:
            val = self._cache.incr(key, delta)

        # python-memcache responds to incr on non-existent keys by
        # raising a ValueError. Cmemcache returns None. In both
        # cases, we should raise a ValueError though.
        except ValueError:
            val = None
        if val is None:
            raise ValueError("Key '%s' not found" % key)

        return val

    def decr(self, key, delta=1):
        try:
            val = self._cache.decr(key, delta)

        # python-memcache responds to decr on non-existent keys by
        # raising a ValueError. Cmemcache returns None. In both
        # cases, we should raise a ValueError though.
        except ValueError:
            val = None
        if val is None:
            raise ValueError("Key '%s' not found" % key)
        return val

    def set_many(self, data, timeout=0):
        safe_data = {}
        for key, value in data.items():
            if isinstance(value, unicode):
                value = value.encode('utf-8')
            safe_data[smart_str(key)] = value
        self._cache.set_multi(safe_data, self._get_memcache_timeout(timeout))

    def delete_many(self, keys):
        self._cache.delete_multi(map(smart_str, keys))

    def clear(self):
        self._cache.flush_all()
</t>
<t tx="newlife.20101101104322.1261"></t>
<t tx="newlife.20101101104322.1262">#!/usr/bin/env python
import sys
import socket
import time
import os
import re
try:
    import cPickle as pickle
except ImportError:
    import pickle

from binascii import crc32   # zlib version is not cross-platform
</t>
<t tx="newlife.20101101104322.1263">def cmemcache_hash(key):
    return((((crc32(key) &amp; 0xffffffff) &gt;&gt; 16) &amp; 0x7fff) or 1)
这个就是这个模块用来加密的东西。</t>
<t tx="newlife.20101101104322.1264">serverHashFunction = cmemcache_hash

def useOldServerHashFunction():
    """Use the old python-memcache server hash function."""
    serverHashFunction = crc32

</t>
<t tx="newlife.20101101104322.1265">try:
    from zlib import compress, decompress
    _supports_compress = True
except ImportError:
    _supports_compress = False
    # quickly define a decompress just in case we recv compressed data.
    def decompress(val):
        raise _Error("received compressed data but I don't support compession (import error)")

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO


__author__    = "Evan Martin &lt;martine@danga.com&gt;"
__version__ = "1.45"
__copyright__ = "Copyright (C) 2003 Danga Interactive"
__license__   = "Python"

SERVER_MAX_KEY_LENGTH = 250
#  Storing values larger than 1MB requires recompiling memcached.  If you do,
#  this value can be changed by doing "memcache.SERVER_MAX_VALUE_LENGTH = N"
#  after importing this module.
SERVER_MAX_VALUE_LENGTH = 1024*1024

class _Error(Exception):
    pass

</t>
<t tx="newlife.20101101104322.1266">try:
    # Only exists in Python 2.4+
    from threading import local
except ImportError:
    # TODO:  add the pure-python local implementation
    class local(object):
        pass


class Client(local):
    """
    Object representing a pool of memcache servers.

    See L{memcache} for an overview.

    In all cases where a key is used, the key can be either:
        1. A simple hashable type (string, integer, etc.).
        2. A tuple of C{(hashvalue, key)}.  This is useful if you want to avoid
        making this module calculate a hash value.  You may prefer, for
        example, to keep all of a given user's objects on the same memcache
        server, so you could use the user's unique id as the hash value.

    @group Setup: __init__, set_servers, forget_dead_hosts, disconnect_all, debuglog
    @group Insertion: set, add, replace, set_multi
    @group Retrieval: get, get_multi
    @group Integers: incr, decr
    @group Removal: delete, delete_multi
    @sort: __init__, set_servers, forget_dead_hosts, disconnect_all, debuglog,\
           set, set_multi, add, replace, get, get_multi, incr, decr, delete, delete_multi
    """
    _FLAG_PICKLE  = 1&lt;&lt;0
    _FLAG_INTEGER = 1&lt;&lt;1
    _FLAG_LONG    = 1&lt;&lt;2
    _FLAG_COMPRESSED = 1&lt;&lt;3

    _SERVER_RETRIES = 10  # how many times to try finding a free server.

    @others
</t>
<t tx="newlife.20101101104322.1267"># exceptions for Client
class MemcachedKeyError(Exception):
    pass
</t>
<t tx="newlife.20101101104322.1268">class MemcachedKeyLengthError(MemcachedKeyError):
    pass
</t>
<t tx="newlife.20101101104322.1269">class MemcachedKeyCharacterError(MemcachedKeyError):
    pass
</t>
<t tx="newlife.20101101104322.1270">class MemcachedKeyNoneError(MemcachedKeyError):
    pass
</t>
<t tx="newlife.20101101104322.1271">class MemcachedKeyTypeError(MemcachedKeyError):
    pass
</t>
<t tx="newlife.20101101104322.1272">class MemcachedStringEncodingError(Exception):
    pass

</t>
<t tx="newlife.20101101104322.1273">def __init__(self, servers, debug=0, pickleProtocol=0,
             pickler=pickle.Pickler, unpickler=pickle.Unpickler,
             pload=None, pid=None, server_max_key_length=SERVER_MAX_KEY_LENGTH,
             server_max_value_length=SERVER_MAX_VALUE_LENGTH):
    """
    Create a new Client object with the given list of servers.

    @param servers: C{servers} is passed to L{set_servers}.
    @param debug: whether to display error messages when a server can't be
    contacted.
    @param pickleProtocol: number to mandate protocol used by (c)Pickle.
    @param pickler: optional override of default Pickler to allow subclassing.
    @param unpickler: optional override of default Unpickler to allow subclassing.
    @param pload: optional persistent_load function to call on pickle loading.
    Useful for cPickle since subclassing isn't allowed.
    @param pid: optional persistent_id function to call on pickle storing.
    Useful for cPickle since subclassing isn't allowed.
    """
    local.__init__(self)
    self.debug = debug
    self.set_servers(servers)
    self.stats = {}
    self.cas_ids = {}

    # Allow users to modify pickling/unpickling behavior
    self.pickleProtocol = pickleProtocol
    self.pickler = pickler
    self.unpickler = unpickler
    self.persistent_load = pload
    self.persistent_id = pid
    self.server_max_key_length = server_max_key_length
    self.server_max_value_length = server_max_value_length

    #  figure out the pickler style
    file = StringIO()
    try:
        pickler = self.pickler(file, protocol = self.pickleProtocol)
        self.picklerIsKeyword = True
    except TypeError:
        self.picklerIsKeyword = False

</t>
<t tx="newlife.20101101104322.1274">def set_servers(self, servers):
    """
    Set the pool of servers used by this client.

    @param servers: an array of servers.
    Servers can be passed in two forms:
        1. Strings of the form C{"host:port"}, which implies a default weight of 1.
        2. Tuples of the form C{("host:port", weight)}, where C{weight} is an integer weight value.
    """
    self.servers = [_Host(s, self.debug) for s in servers]
    self._init_buckets()
#这一句是关键阿，关注_init_buckets()

</t>
<t tx="newlife.20101101104322.1275">def get_stats(self):
    '''Get statistics from each of the servers.

    @return: A list of tuples ( server_identifier, stats_dictionary ).
        The dictionary contains a number of name/value pairs specifying
        the name of the status field and the string value associated with
        it.  The values are not converted from strings.
    '''
    data = []
    for s in self.servers:
        if not s.connect(): continue
        if s.family == socket.AF_INET:
            name = '%s:%s (%s)' % ( s.ip, s.port, s.weight )
        else:
            name = 'unix:%s (%s)' % ( s.address, s.weight )
        s.send_cmd('stats')
        serverData = {}
        data.append(( name, serverData ))
        readline = s.readline
        while 1:
            line = readline()
            if not line or line.strip() == 'END': break
            stats = line.split(' ', 2)
            serverData[stats[1]] = stats[2]

    return(data)

</t>
<t tx="newlife.20101101104322.1276">def get_slabs(self):
    data = []
    for s in self.servers:
        if not s.connect(): continue
        if s.family == socket.AF_INET:
            name = '%s:%s (%s)' % ( s.ip, s.port, s.weight )
        else:
            name = 'unix:%s (%s)' % ( s.address, s.weight )
        serverData = {}
        data.append(( name, serverData ))
        s.send_cmd('stats items')
        readline = s.readline
        while 1:
            line = readline()
            if not line or line.strip() == 'END': break
            item = line.split(' ', 2)
            #0 = STAT, 1 = ITEM, 2 = Value
            slab = item[1].split(':', 2)
            #0 = items, 1 = Slab #, 2 = Name
            if slab[1] not in serverData:
                serverData[slab[1]] = {}
            serverData[slab[1]][slab[2]] = item[2]
    return data

</t>
<t tx="newlife.20101101104322.1277">def flush_all(self):
    'Expire all data currently in the memcache servers.'
    for s in self.servers:
        if not s.connect(): continue
        s.send_cmd('flush_all')
        s.expect("OK")

</t>
<t tx="newlife.20101101104322.1278">def debuglog(self, str):
    if self.debug:
        sys.stderr.write("MemCached: %s\n" % str)

</t>
<t tx="newlife.20101101104322.1279">def _statlog(self, func):
    if func not in self.stats:
        self.stats[func] = 1
    else:
        self.stats[func] += 1

</t>
<t tx="newlife.20101101104322.1280">def forget_dead_hosts(self):
    """
    Reset every host in the pool to an "alive" state.
    """
    for s in self.servers:
        s.deaduntil = 0

</t>
<t tx="newlife.20101101104322.1281">def _init_buckets(self):
    self.buckets = []
    for server in self.servers:
        for i in range(server.weight):
            self.buckets.append(server)

##这里出现了一个叫weight的属性，好，现在猜一下，这个东西是干啥的，首先是一个int的数值，
##这里的server是一台缓存服务器，server.weight应该就是这台机器上缓存的对象的个数，，有待验证。
##self.buckets里保存的是，一个一个的server??,先去看weight是如何定义的。
——————————————————————————————————————————————————————————————————————————————————
结论：简单的说;self.buckets就是一个_Host对象的列表list。没有了。。</t>
<t tx="newlife.20101101104322.1282">def _get_server(self, key):
    if isinstance(key, tuple):
        serverhash, key = key
    else:
        serverhash = serverHashFunction(key)

    for i in range(Client._SERVER_RETRIES):
        server = self.buckets[serverhash % len(self.buckets)]
        if server.connect():
            #print "(using server %s)" % server,
            return server, key
        serverhash = serverHashFunction(str(serverhash) + str(i))
    return None, None

</t>
<t tx="newlife.20101101104322.1283">def disconnect_all(self):
    for s in self.servers:
        s.close_socket()

</t>
<t tx="newlife.20101101104322.1284">def delete_multi(self, keys, time=0, key_prefix=''):
    '''
    Delete multiple keys in the memcache doing just one query.

    &gt;&gt;&gt; notset_keys = mc.set_multi({'key1' : 'val1', 'key2' : 'val2'})
    &gt;&gt;&gt; mc.get_multi(['key1', 'key2']) == {'key1' : 'val1', 'key2' : 'val2'}
    1
    &gt;&gt;&gt; mc.delete_multi(['key1', 'key2'])
    1
    &gt;&gt;&gt; mc.get_multi(['key1', 'key2']) == {}
    1


    This method is recommended over iterated regular L{delete}s as it reduces total latency, since
    your app doesn't have to wait for each round-trip of L{delete} before sending
    the next one.

    @param keys: An iterable of keys to clear
    @param time: number of seconds any subsequent set / update commands should fail. Defaults to 0 for no delay.
    @param key_prefix:  Optional string to prepend to each key when sending to memcache.
        See docs for L{get_multi} and L{set_multi}.

    @return: 1 if no failure in communication with any memcacheds.
    @rtype: int

    '''

    self._statlog('delete_multi')

    server_keys, prefixed_to_orig_key = self._map_and_prefix_keys(keys, key_prefix)

    # send out all requests on each server before reading anything
    dead_servers = []

    rc = 1
    for server in server_keys.iterkeys():
        bigcmd = []
        write = bigcmd.append
        if time != None:
             for key in server_keys[server]: # These are mangled keys
                 write("delete %s %d\r\n" % (key, time))
        else:
            for key in server_keys[server]: # These are mangled keys
              write("delete %s\r\n" % key)
        try:
            server.send_cmds(''.join(bigcmd))
        except socket.error, msg:
            rc = 0
            if isinstance(msg, tuple): msg = msg[1]
            server.mark_dead(msg)
            dead_servers.append(server)

    # if any servers died on the way, don't expect them to respond.
    for server in dead_servers:
        del server_keys[server]

    notstored = [] # original keys.
    for server, keys in server_keys.iteritems():
        try:
            for key in keys:
                server.expect("DELETED")
        except socket.error, msg:
            if isinstance(msg, tuple): msg = msg[1]
            server.mark_dead(msg)
            rc = 0
    return rc

</t>
<t tx="newlife.20101101104322.1285">def delete(self, key, time=0):
    '''Deletes a key from the memcache.

    @return: Nonzero on success.
    @param time: number of seconds any subsequent set / update commands
    should fail. Defaults to 0 for no delay.
    @rtype: int
    '''
    self.check_key(key)
    server, key = self._get_server(key)
    if not server:
        return 0
    self._statlog('delete')
    if time != None:
        cmd = "delete %s %d" % (key, time)
    else:
        cmd = "delete %s" % key

    try:
        server.send_cmd(cmd)
        server.expect("DELETED")
    except socket.error, msg:
        if isinstance(msg, tuple): msg = msg[1]
        server.mark_dead(msg)
        return 0
    return 1

</t>
<t tx="newlife.20101101104322.1286">def incr(self, key, delta=1):
    """
    Sends a command to the server to atomically increment the value
    for C{key} by C{delta}, or by 1 if C{delta} is unspecified.
    Returns None if C{key} doesn't exist on server, otherwise it
    returns the new value after incrementing.

    Note that the value for C{key} must already exist in the memcache,
    and it must be the string representation of an integer.

    &gt;&gt;&gt; mc.set("counter", "20")  # returns 1, indicating success
    1
    &gt;&gt;&gt; mc.incr("counter")
    21
    &gt;&gt;&gt; mc.incr("counter")
    22

    Overflow on server is not checked.  Be aware of values approaching
    2**32.  See L{decr}.

    @param delta: Integer amount to increment by (should be zero or greater).
    @return: New value after incrementing.
    @rtype: int
    """
    return self._incrdecr("incr", key, delta)

</t>
<t tx="newlife.20101101104322.1287">def decr(self, key, delta=1):
    """
    Like L{incr}, but decrements.  Unlike L{incr}, underflow is checked and
    new values are capped at 0.  If server value is 1, a decrement of 2
    returns 0, not -1.

    @param delta: Integer amount to decrement by (should be zero or greater).
    @return: New value after decrementing.
    @rtype: int
    """
    return self._incrdecr("decr", key, delta)

</t>
<t tx="newlife.20101101104322.1288">def _incrdecr(self, cmd, key, delta):
    self.check_key(key)
    server, key = self._get_server(key)
    if not server:
        return 0
    self._statlog(cmd)
    cmd = "%s %s %d" % (cmd, key, delta)
    try:
        server.send_cmd(cmd)
        line = server.readline()
        if line.strip() =='NOT_FOUND': return None
        return int(line)
    except socket.error, msg:
        if isinstance(msg, tuple): msg = msg[1]
        server.mark_dead(msg)
        return None

</t>
<t tx="newlife.20101101104322.1289">def add(self, key, val, time = 0, min_compress_len = 0):
    '''
    Add new key with value.

    Like L{set}, but only stores in memcache if the key doesn't already exist.

    @return: Nonzero on success.
    @rtype: int
    '''
    return self._set("add", key, val, time, min_compress_len)

</t>
<t tx="newlife.20101101104322.1290">def append(self, key, val, time=0, min_compress_len=0):
    '''Append the value to the end of the existing key's value.

    Only stores in memcache if key already exists.
    Also see L{prepend}.

    @return: Nonzero on success.
    @rtype: int
    '''
    return self._set("append", key, val, time, min_compress_len)

</t>
<t tx="newlife.20101101104322.1291">def prepend(self, key, val, time=0, min_compress_len=0):
    '''Prepend the value to the beginning of the existing key's value.

    Only stores in memcache if key already exists.
    Also see L{append}.

    @return: Nonzero on success.
    @rtype: int
    '''
    return self._set("prepend", key, val, time, min_compress_len)

</t>
<t tx="newlife.20101101104322.1292">def replace(self, key, val, time=0, min_compress_len=0):
    '''Replace existing key with value.

    Like L{set}, but only stores in memcache if the key already exists.
    The opposite of L{add}.

    @return: Nonzero on success.
    @rtype: int
    '''
    return self._set("replace", key, val, time, min_compress_len)

</t>
<t tx="newlife.20101101104322.1293">def set(self, key, val, time=0, min_compress_len=0):
    '''Unconditionally sets a key to a given value in the memcache.
    在memcache中，给一个key设置给定的值，
    The C{key} can optionally be an tuple, with the first element
    being the server hash value and the second being the key.
    If you want to avoid making this module calculate a hash value.
    You may prefer, for example, to keep all of a given user's objects
    on the same memcache server, so you could use the user's unique
    id as the hash value.
    
    @return: Nonzero on success.
    @rtype: int
    @param time: Tells memcached the time which this value should expire, either
    as a delta number of seconds, or an absolute unix time-since-the-epoch
    value. See the memcached protocol docs section "Storage Commands"
    for more info on &lt;exptime&gt;. We default to 0 == cache forever.
    @param min_compress_len: The threshold length to kick in auto-compression
    of the value using the zlib.compress() routine. If the value being cached is
    a string, then the length of the string is measured, else if the value is an
    object, then the length of the pickle result is measured. If the resulting
    attempt at compression yeilds a larger string than the input, then it is
    discarded. For backwards compatability, this parameter defaults to 0,
    indicating don't ever try to compress.
    '''
    return self._set("set", key, val, time, min_compress_len)


</t>
<t tx="newlife.20101101104322.1294">def cas(self, key, val, time=0, min_compress_len=0):
    '''Sets a key to a given value in the memcache if it hasn't been
    altered since last fetched. (See L{gets}).

    The C{key} can optionally be an tuple, with the first element
    being the server hash value and the second being the key.
    If you want to avoid making this module calculate a hash value.
    You may prefer, for example, to keep all of a given user's objects
    on the same memcache server, so you could use the user's unique
    id as the hash value.

    @return: Nonzero on success.
    @rtype: int
    @param time: Tells memcached the time which this value should expire,
    either as a delta number of seconds, or an absolute unix
    time-since-the-epoch value. See the memcached protocol docs section
    "Storage Commands" for more info on &lt;exptime&gt;. We default to
    0 == cache forever.
    @param min_compress_len: The threshold length to kick in
    auto-compression of the value using the zlib.compress() routine. If
    the value being cached is a string, then the length of the string is
    measured, else if the value is an object, then the length of the
    pickle result is measured. If the resulting attempt at compression
    yeilds a larger string than the input, then it is discarded. For
    backwards compatability, this parameter defaults to 0, indicating
    don't ever try to compress.
    '''
    return self._set("cas", key, val, time, min_compress_len)


</t>
<t tx="newlife.20101101104322.1295">def _map_and_prefix_keys(self, key_iterable, key_prefix):
    """Compute the mapping of server (_Host instance) -&gt; list of keys to stuff onto that server, as well as the mapping of
    prefixed key -&gt; original key.


    """
    # Check it just once ...
    key_extra_len=len(key_prefix)
    if key_prefix:
        self.check_key(key_prefix)

    # server (_Host) -&gt; list of unprefixed server keys in mapping
    server_keys = {}

    prefixed_to_orig_key = {}
    # build up a list for each server of all the keys we want.
    for orig_key in key_iterable:
        if isinstance(orig_key, tuple):
            # Tuple of hashvalue, key ala _get_server(). Caller is essentially telling us what server to stuff this on.
            # Ensure call to _get_server gets a Tuple as well.
            str_orig_key = str(orig_key[1])
            server, key = self._get_server((orig_key[0], key_prefix + str_orig_key)) # Gotta pre-mangle key before hashing to a server. Returns the mangled key.
        else:
            str_orig_key = str(orig_key) # set_multi supports int / long keys.
            server, key = self._get_server(key_prefix + str_orig_key)

        # Now check to make sure key length is proper ...
        self.check_key(str_orig_key, key_extra_len=key_extra_len)

        if not server:
            continue

        if server not in server_keys:
            server_keys[server] = []
        server_keys[server].append(key)
        prefixed_to_orig_key[key] = orig_key

    return (server_keys, prefixed_to_orig_key)

</t>
<t tx="newlife.20101101104322.1296">def set_multi(self, mapping, time=0, key_prefix='', min_compress_len=0):
    '''
    Sets multiple keys in the memcache doing just one query.

    &gt;&gt;&gt; notset_keys = mc.set_multi({'key1' : 'val1', 'key2' : 'val2'})
    &gt;&gt;&gt; mc.get_multi(['key1', 'key2']) == {'key1' : 'val1', 'key2' : 'val2'}
    1


    This method is recommended over regular L{set} as it lowers the number of
    total packets flying around your network, reducing total latency, since
    your app doesn't have to wait for each round-trip of L{set} before sending
    the next one.

    @param mapping: A dict of key/value pairs to set.
    @param time: Tells memcached the time which this value should expire, either
    as a delta number of seconds, or an absolute unix time-since-the-epoch
    value. See the memcached protocol docs section "Storage Commands"
    for more info on &lt;exptime&gt;. We default to 0 == cache forever.
    @param key_prefix:  Optional string to prepend to each key when sending to memcache. Allows you to efficiently stuff these keys into a pseudo-namespace in memcache:
        &gt;&gt;&gt; notset_keys = mc.set_multi({'key1' : 'val1', 'key2' : 'val2'}, key_prefix='subspace_')
        &gt;&gt;&gt; len(notset_keys) == 0
        True
        &gt;&gt;&gt; mc.get_multi(['subspace_key1', 'subspace_key2']) == {'subspace_key1' : 'val1', 'subspace_key2' : 'val2'}
        True

        Causes key 'subspace_key1' and 'subspace_key2' to be set. Useful in conjunction with a higher-level layer which applies namespaces to data in memcache.
        In this case, the return result would be the list of notset original keys, prefix not applied.

    @param min_compress_len: The threshold length to kick in auto-compression
    of the value using the zlib.compress() routine. If the value being cached is
    a string, then the length of the string is measured, else if the value is an
    object, then the length of the pickle result is measured. If the resulting
    attempt at compression yeilds a larger string than the input, then it is
    discarded. For backwards compatability, this parameter defaults to 0,
    indicating don't ever try to compress.
    @return: List of keys which failed to be stored [ memcache out of memory, etc. ].
    @rtype: list

    '''

    self._statlog('set_multi')



    server_keys, prefixed_to_orig_key = self._map_and_prefix_keys(mapping.iterkeys(), key_prefix)

    # send out all requests on each server before reading anything
    dead_servers = []

    for server in server_keys.iterkeys():
        bigcmd = []
        write = bigcmd.append
        try:
            for key in server_keys[server]: # These are mangled keys
                store_info = self._val_to_store_info(mapping[prefixed_to_orig_key[key]], min_compress_len)
                write("set %s %d %d %d\r\n%s\r\n" % (key, store_info[0], time, store_info[1], store_info[2]))
            server.send_cmds(''.join(bigcmd))
        except socket.error, msg:
            if isinstance(msg, tuple): msg = msg[1]
            server.mark_dead(msg)
            dead_servers.append(server)

    # if any servers died on the way, don't expect them to respond.
    for server in dead_servers:
        del server_keys[server]

    #  short-circuit if there are no servers, just return all keys
    if not server_keys: return(mapping.keys())

    notstored = [] # original keys.
    for server, keys in server_keys.iteritems():
        try:
            for key in keys:
                line = server.readline()
                if line == 'STORED':
                    continue
                else:
                    notstored.append(prefixed_to_orig_key[key]) #un-mangle.
        except (_Error, socket.error), msg:
            if isinstance(msg, tuple): msg = msg[1]
            server.mark_dead(msg)
    return notstored

</t>
<t tx="newlife.20101101104322.1297">def _val_to_store_info(self, val, min_compress_len):
    """
       Transform val to a storable representation, returning a tuple of the flags, the length of the new value, and the new value itself.
    """
    flags = 0
    if isinstance(val, str):
        pass
    elif isinstance(val, int):
        flags |= Client._FLAG_INTEGER
        val = "%d" % val
        # force no attempt to compress this silly string.
        min_compress_len = 0
    elif isinstance(val, long):
        flags |= Client._FLAG_LONG
        val = "%d" % val
        # force no attempt to compress this silly string.
        min_compress_len = 0
    else:
        flags |= Client._FLAG_PICKLE
        file = StringIO()
        if self.picklerIsKeyword:
            pickler = self.pickler(file, protocol = self.pickleProtocol)
        else:
            pickler = self.pickler(file, self.pickleProtocol)
        if self.persistent_id:
            pickler.persistent_id = self.persistent_id
        pickler.dump(val)
        val = file.getvalue()

    lv = len(val)
    # We should try to compress if min_compress_len &gt; 0 and we could
    # import zlib and this string is longer than our min threshold.
    if min_compress_len and _supports_compress and lv &gt; min_compress_len:
        comp_val = compress(val)
        # Only retain the result if the compression result is smaller
        # than the original.
        if len(comp_val) &lt; lv:
            flags |= Client._FLAG_COMPRESSED
            val = comp_val

    #  silently do not store if value length exceeds maximum
    if self.server_max_value_length != 0 and \
       len(val) &gt;= self.server_max_value_length: return(0)

    return (flags, len(val), val)

</t>
<t tx="newlife.20101101104322.1298">def _set(self, cmd, key, val, time, min_compress_len = 0):
    self.check_key(key)
    #这个方法是干啥用的？答：检查key的合理性的。
    server, key = self._get_server(key)
    ##
    if not server:
        return 0

    self._statlog(cmd)

    store_info = self._val_to_store_info(val, min_compress_len)
    if not store_info: return(0)

    if cmd == 'cas':
        if key not in self.cas_ids:
            return self._set('set', key, val, time, min_compress_len)
        fullcmd = "%s %s %d %d %d %d\r\n%s" % (
                cmd, key, store_info[0], time, store_info[1],
                self.cas_ids[key], store_info[2])
    else:
        fullcmd = "%s %s %d %d %d\r\n%s" % (
                cmd, key, store_info[0], time, store_info[1], store_info[2])

    try:
        server.send_cmd(fullcmd)
        return(server.expect("STORED") == "STORED")
    except socket.error, msg:
        if isinstance(msg, tuple): msg = msg[1]
        server.mark_dead(msg)
    return 0

</t>
<t tx="newlife.20101101104322.1299">def _get(self, cmd, key):
    self.check_key(key)
    server, key = self._get_server(key)
    if not server:
        return None

    self._statlog(cmd)

    try:
        server.send_cmd("%s %s" % (cmd, key))
        rkey = flags = rlen = cas_id = None
        if cmd == 'gets':
            rkey, flags, rlen, cas_id, = self._expect_cas_value(server)
            if rkey:
                self.cas_ids[rkey] = cas_id
        else:
            rkey, flags, rlen, = self._expectvalue(server)

        if not rkey:
            return None
        value = self._recv_value(server, flags, rlen)
        server.expect("END")
    except (_Error, socket.error), msg:
        if isinstance(msg, tuple): msg = msg[1]
        server.mark_dead(msg)
        return None
    return value

</t>
<t tx="newlife.20101101104322.1300">def get(self, key):
    '''Retrieves a key from the memcache.

    @return: The value or None.
    '''
    return self._get('get', key)

</t>
<t tx="newlife.20101101104322.1301">def gets(self, key):
    '''Retrieves a key from the memcache. Used in conjunction with 'cas'.

    @return: The value or None.
    '''
    return self._get('gets', key)

</t>
<t tx="newlife.20101101104322.1302">def get_multi(self, keys, key_prefix=''):
    '''
    Retrieves multiple keys from the memcache doing just one query.

    &gt;&gt;&gt; success = mc.set("foo", "bar")
    &gt;&gt;&gt; success = mc.set("baz", 42)
    &gt;&gt;&gt; mc.get_multi(["foo", "baz", "foobar"]) == {"foo": "bar", "baz": 42}
    1
    &gt;&gt;&gt; mc.set_multi({'k1' : 1, 'k2' : 2}, key_prefix='pfx_') == []
    1

    This looks up keys 'pfx_k1', 'pfx_k2', ... . Returned dict will just have unprefixed keys 'k1', 'k2'.
    &gt;&gt;&gt; mc.get_multi(['k1', 'k2', 'nonexist'], key_prefix='pfx_') == {'k1' : 1, 'k2' : 2}
    1

    get_mult [ and L{set_multi} ] can take str()-ables like ints / longs as keys too. Such as your db pri key fields.
    They're rotored through str() before being passed off to memcache, with or without the use of a key_prefix.
    In this mode, the key_prefix could be a table name, and the key itself a db primary key number.

    &gt;&gt;&gt; mc.set_multi({42: 'douglass adams', 46 : 'and 2 just ahead of me'}, key_prefix='numkeys_') == []
    1
    &gt;&gt;&gt; mc.get_multi([46, 42], key_prefix='numkeys_') == {42: 'douglass adams', 46 : 'and 2 just ahead of me'}
    1

    This method is recommended over regular L{get} as it lowers the number of
    total packets flying around your network, reducing total latency, since
    your app doesn't have to wait for each round-trip of L{get} before sending
    the next one.

    See also L{set_multi}.

    @param keys: An array of keys.
    @param key_prefix: A string to prefix each key when we communicate with memcache.
        Facilitates pseudo-namespaces within memcache. Returned dictionary keys will not have this prefix.
    @return:  A dictionary of key/value pairs that were available. If key_prefix was provided, the keys in the retured dictionary will not have it present.

    '''

    self._statlog('get_multi')

    server_keys, prefixed_to_orig_key = self._map_and_prefix_keys(keys, key_prefix)

    # send out all requests on each server before reading anything
    dead_servers = []
    for server in server_keys.iterkeys():
        try:
            server.send_cmd("get %s" % " ".join(server_keys[server]))
        except socket.error, msg:
            if isinstance(msg, tuple): msg = msg[1]
            server.mark_dead(msg)
            dead_servers.append(server)

    # if any servers died on the way, don't expect them to respond.
    for server in dead_servers:
        del server_keys[server]

    retvals = {}
    for server in server_keys.iterkeys():
        try:
            line = server.readline()
            while line and line != 'END':
                rkey, flags, rlen = self._expectvalue(server, line)
                #  Bo Yang reports that this can sometimes be None
                if rkey is not None:
                    val = self._recv_value(server, flags, rlen)
                    retvals[prefixed_to_orig_key[rkey]] = val   # un-prefix returned key.
                line = server.readline()
        except (_Error, socket.error), msg:
            if isinstance(msg, tuple): msg = msg[1]
            server.mark_dead(msg)
    return retvals

</t>
<t tx="newlife.20101101104322.1303">def _expect_cas_value(self, server, line=None):
    if not line:
        line = server.readline()

    if line[:5] == 'VALUE':
        resp, rkey, flags, len, cas_id = line.split()
        return (rkey, int(flags), int(len), int(cas_id))
    else:
        return (None, None, None, None)

</t>
<t tx="newlife.20101101104322.1304">def _expectvalue(self, server, line=None):
    if not line:
        line = server.readline()

    if line[:5] == 'VALUE':
        resp, rkey, flags, len = line.split()
        flags = int(flags)
        rlen = int(len)
        return (rkey, flags, rlen)
    else:
        return (None, None, None)

</t>
<t tx="newlife.20101101104322.1305">def _recv_value(self, server, flags, rlen):
    rlen += 2 # include \r\n
    buf = server.recv(rlen)
    if len(buf) != rlen:
        raise _Error("received %d bytes when expecting %d"
                % (len(buf), rlen))

    if len(buf) == rlen:
        buf = buf[:-2]  # strip \r\n

    if flags &amp; Client._FLAG_COMPRESSED:
        buf = decompress(buf)

    if  flags == 0 or flags == Client._FLAG_COMPRESSED:
        # Either a bare string or a compressed string now decompressed...
        val = buf
    elif flags &amp; Client._FLAG_INTEGER:
        val = int(buf)
    elif flags &amp; Client._FLAG_LONG:
        val = long(buf)
    elif flags &amp; Client._FLAG_PICKLE:
        try:
            file = StringIO(buf)
            unpickler = self.unpickler(file)
            if self.persistent_load:
                unpickler.persistent_load = self.persistent_load
            val = unpickler.load()
        except Exception, e:
            self.debuglog('Pickle error: %s\n' % e)
            val = None
    else:
        self.debuglog("unknown flags on get: %x\n" % flags)

    return val

</t>
<t tx="newlife.20101101104322.1306">def check_key(self, key, key_extra_len=0):
    """Checks sanity of key.  Fails if:
        Key length is &gt; SERVER_MAX_KEY_LENGTH (Raises MemcachedKeyLength).
        Contains control characters  (Raises MemcachedKeyCharacterError).
        Is not a string (Raises MemcachedStringEncodingError)
        Is an unicode string (Raises MemcachedStringEncodingError)
        Is not a string (Raises MemcachedKeyError)
        Is None (Raises MemcachedKeyError)
        检查输入的key的合理性，以下情况不合格：
        长度大于设定值；
        包含关键字（有哪些？）；
        不是string；
        不是unicode；
        是None；
    """
    if isinstance(key, tuple): key = key[1]
    if not key:
        raise Client.MemcachedKeyNoneError("Key is None")
    if isinstance(key, unicode):
        raise Client.MemcachedStringEncodingError(
                "Keys must be str()'s, not unicode.  Convert your unicode "
                "strings using mystring.encode(charset)!")
    if not isinstance(key, str):
        raise Client.MemcachedKeyTypeError("Key must be str()'s")

    if isinstance(key, basestring):
        if self.server_max_key_length != 0 and \
            len(key) + key_extra_len &gt; self.server_max_key_length:
            raise Client.MemcachedKeyLengthError("Key length is &gt; %s"
                     % self.server_max_key_length)
        for char in key:
            if ord(char) &lt; 33 or ord(char) == 127:
                raise Client.MemcachedKeyCharacterError(
                        "Control characters not allowed")
            #这些值ms没有一个常见的。。

</t>
<t tx="newlife.20101101104322.1307">class _Host(object):
    _DEAD_RETRY = 30  # number of seconds before retrying a dead server.
    _SOCKET_TIMEOUT = 3  #  number of seconds before sockets timeout.

    @others
</t>
<t tx="newlife.20101101104322.1308">def __init__(self, host, debug=0):
    self.debug = debug
    if isinstance(host, tuple):
        host, self.weight = host##这个处理的是啥情况，
    else:
        self.weight = 1
    #正常的时候我们这里的接收的host的参数是一个类似“127.0.0.1：11211”的参数，由此可以推断出_Host的实例的weight属性一般都是1.并不是之前猜测的包含服务器的个数。
    #  parse the connection string
    m = re.match(r'^(?P&lt;proto&gt;unix):(?P&lt;path&gt;.*)$', host)
    if not m:
        m = re.match(r'^(?P&lt;proto&gt;inet):'
                r'(?P&lt;host&gt;[^:]+)(:(?P&lt;port&gt;[0-9]+))?$', host)
    if not m: m = re.match(r'^(?P&lt;host&gt;[^:]+):(?P&lt;port&gt;[0-9]+)$', host)
    if not m:
        raise ValueError('Unable to parse connection string: "%s"' % host)

    hostData = m.groupdict()
    ##这里的hostData返回的是一个字典：类似"{'host': '127.0.0.1', 'port': '11211'}"
    if hostData.get('proto') == 'unix':
        self.family = socket.AF_UNIX
        self.address = hostData['path']
    else:
        self.family = socket.AF_INET
        self.ip = hostData['host']
        self.port = int(hostData.get('port', 11211))
        self.address = ( self.ip, self.port )

    self.deaduntil = 0
    self.socket = None

    self.buffer = ''

</t>
<t tx="newlife.20101101104322.1309">def debuglog(self, str):
    if self.debug:
        sys.stderr.write("MemCached: %s\n" % str)

</t>
<t tx="newlife.20101101104322.1310">def _check_dead(self):
    if self.deaduntil and self.deaduntil &gt; time.time():
        return 1
    self.deaduntil = 0
    return 0

</t>
<t tx="newlife.20101101104322.1311">def connect(self):
    if self._get_socket():
        return 1
    return 0

</t>
<t tx="newlife.20101101104322.1312">def mark_dead(self, reason):
    self.debuglog("MemCache: %s: %s.  Marking dead." % (self, reason))
    self.deaduntil = time.time() + _Host._DEAD_RETRY
    self.close_socket()

</t>
<t tx="newlife.20101101104322.1313">def _get_socket(self):
    if self._check_dead():
        return None
    if self.socket:
        return self.socket
    s = socket.socket(self.family, socket.SOCK_STREAM)
    if hasattr(s, 'settimeout'): s.settimeout(self._SOCKET_TIMEOUT)
    try:
        s.connect(self.address)
    except socket.timeout, msg:
        self.mark_dead("connect: %s" % msg)
        return None
    except socket.error, msg:
        if isinstance(msg, tuple): msg = msg[1]
        self.mark_dead("connect: %s" % msg[1])
        return None
    self.socket = s
    self.buffer = ''
    return s

</t>
<t tx="newlife.20101101104322.1314">def close_socket(self):
    if self.socket:
        self.socket.close()
        self.socket = None

</t>
<t tx="newlife.20101101104322.1315">def send_cmd(self, cmd):
    self.socket.sendall(cmd + '\r\n')

</t>
<t tx="newlife.20101101104322.1316">def send_cmds(self, cmds):
    """ cmds already has trailing \r\n's applied """
    self.socket.sendall(cmds)

</t>
<t tx="newlife.20101101104322.1317">def readline(self):
    buf = self.buffer
    recv = self.socket.recv
    while True:
        index = buf.find('\r\n')
        if index &gt;= 0:
            break
        data = recv(4096)
        if not data:
            self.mark_dead('Connection closed while reading from %s'
                    % repr(self))
            self.buffer = ''
            return None
        buf += data
    self.buffer = buf[index+2:]
    return buf[:index]

</t>
<t tx="newlife.20101101104322.1318">def expect(self, text):
    line = self.readline()
    if line != text:
        self.debuglog("while expecting '%s', got unexpected response '%s'"
                % (text, line))
    return line

</t>
<t tx="newlife.20101101104322.1319">def recv(self, rlen):
    self_socket_recv = self.socket.recv
    buf = self.buffer
    while len(buf) &lt; rlen:
        foo = self_socket_recv(max(rlen - len(buf), 4096))
        buf += foo
        if not foo:
            raise _Error( 'Read %d bytes, expecting %d, '
                    'read returned 0 length bytes' % ( len(buf), rlen ))
    self.buffer = buf[rlen:]
    return buf[:rlen]

</t>
<t tx="newlife.20101101104322.1320">def __str__(self):
    d = ''
    if self.deaduntil:
        d = " (dead until %d)" % self.deaduntil

    if self.family == socket.AF_INET:
        return "inet:%s:%d%s" % (self.address[0], self.address[1], d)
    else:
        return "unix:%s%s" % (self.address, d)


</t>
<t tx="newlife.20101101104322.1321">def _doctest():
    import doctest, memcache
    servers = ["127.0.0.1:11211"]
    mc = Client(servers, debug=1)
    globs = {"mc": mc}
    return doctest.testmod(memcache, globs=globs)

</t>
<t tx="newlife.20101101104322.1322">最近看了一下memcached，重点分析了下python-memcached这个类库,仔细看了下django种处理memcached的部分。
看了一些资料，记录下，

关于django的memcache部分，可以在文档上有详细的介绍，具体的操作流程，比如开启服务，设置memcached服务器，等等。

我们关注隐藏在表面下的东西，


所有的一切都是从
    #mc = memcache.Client(['127.0.0.1:11211'], debug=0)
这句开始的，
整个包中关键的类有2个，Client()和_Host()这样2个类，

Client，接收了一个tuple的参数，
    在这个类初始化的时候调用：
        #self.set_servers(servers)
而在set_servers这个方法中调用_host这个类，具体代码如下：
    #self.servers = [_Host(s, self.debug) for s in servers]
    #self._init_buckets()
现在我们去看_Host()这个类的__init__都作了什么：
    #其实啥也没做。。就是解析了下传入的参数.
    #。。你还指望他作啥阿
下面回答前一章提到的问题：
    @@self._init_buckets(),返回的一个由_Host对象组成的列表。
    
数据操作：
    #mc.set("some_key", "Some value")
    </t>
<t tx="newlife.20101101104322.1323">Django’s cache framework

A fundamental trade-off in dynamic Web sites is, well, they’re dynamic. Each time a user requests a page, the Web server makes all sorts of calculations – from database queries to template rendering to business logic – to create the page that your site’s visitor sees. This is a lot more expensive, from a processing-overhead perspective, than your standard read-a-file-off-the-filesystem server arrangement.

动态网站一个根本的权衡，就是他是动态的，每次一个用户请求一个页面，服务区都要进行各种各样的计算，从数据库查询到模板渲染再到业务逻辑，来构建用户想要看到的页面。从处理负载的角度看，这远比从文件系统度读取文件要大的多。

For most Web applications, this overhead isn’t a big deal. Most Web applications aren’t washingtonpost.com or slashdot.org; they’re simply small- to medium-sized sites with so-so traffic. But for medium- to high-traffic sites, it’s essential to cut as much overhead as possible.

对于大部分的web应用。这些负载不是神马大问题，他们都不是新浪阿或者开心校内之流。他们的规模比较小，没有这么大的流量，但是对于中型高负载的网站，负载要尽可能的减少。

That’s where caching comes in.
缓存来了，

To cache something is to save the result of an expensive calculation so that you don’t have to perform the calculation next time. Here’s some pseudocode explaining how this would work for a dynamically generated Web page:
    
缓存就是保存一个耗时耗力的计算结果，这样下次需要的时候就不用执行查询了。下面的为马解释缓存如何工作来生成动态页面。

given a URL, try finding that page in the cache
if the page is in the cache:
    return the cached page
else:
    generate the page
    save the generated page in the cache (for next time)
    return the generated page
请求一个url，看看结果在不在缓存里：
在：
    返回缓存的结果
不在：
    生成页面，放到缓存里（下次用）
    返回结果。
        
Django comes with a robust cache system that lets you save dynamic pages so they do not have to be calculated for each request. For convenience, Django offers different levels of cache granularity: You can cache the output of specific views, you can cache only the pieces that are difficult to produce, or you can cache your entire site.

Django内置了一个强大的缓存系统，是我们可以生成动态页面而不用每次请求都计算生成。Django提供了不同层次的缓存粒度，你可以缓存指定view的结果，或者难以生成的片段，或者整个站点。


Django also works well with "upstream" caches, such as Squid (http://www.squid-cache.org/) and browser-based caches. These are the types of caches that you do not directly control but to which you can provide hints (via HTTP headers) about which parts of your site should be cached, and how.
这个和我们没关系，。

Setting up the cache

The cache system requires a small amount of setup. Namely, you have to tell it where your cached data should live -- whether in a database, on the filesystem or directly in memory. This is an important decision that affects your cache is performance; yes, some cache types are faster than others.

缓存系统需要一些设置，你需要告诉他在哪里缓存，数据库？文件系统？内存？这是个很重要的决策，可以影响你的性能，

Your cache preference goes in the CACHE_BACKEND setting in your settings file. Here is an explanation of all available values for CACHE_BACKEND.

在settings文件里设置CACHE_BACKEND，

Memcached
重点关注，我们就用这个
By far the fastest, most efficient type of cache available to Django, Memcached is an entirely memory-based cache framework originally developed to handle high loads at LiveJournal.com and subsequently open-sourced by Danga Interactive. It is used by sites such as Facebook and Wikipedia to reduce database access and dramatically increase site performance.
memcached就是很好，很多人用。

Memcached is available for free at http://danga.com/memcached/ . It runs as a daemon and is allotted a specified amount of RAM. All it does is provide a fast interface for adding, retrieving and deleting arbitrary data in the cache. All data is stored directly in memory, so there is no overhead of database or filesystem usage.

直接放内存里，木有数据库和文件系统的负载

After installing Memcached itself, you will need to install python-memcached, which provides Python bindings to Memcached. This is available at ftp://ftp.tummy.com/pub/python-memcached/

安装Memcached,安装python-memcached.

Changed in Django 1.2: In Django 1.0 and 1.1, you could also use cmemcache as a binding. However, support for this library was deprecated in 1.2 due to a lack of maintenance on the cmemcache library itself. Support for cmemcache will be removed completely in Django 1.4.
To use Memcached with Django, set CACHE_BACKEND to memcached://ip:port/, where ip is the IP address of the Memcached daemon and port is the port on which Memcached is running.
就用python-memcached,以前那个废弃了,不推荐用.

In this example, Memcached is running on localhost (127.0.0.1) port 11211:
下面的例子中Memcached运行在localhost的11211端口。
CACHE_BACKEND = 'memcached://127.0.0.1:11211/'

One excellent feature of Memcached is its ability to share cache over multiple servers. This means you can run Memcached daemons on multiple machines, and the program will treat the group of machines as a single cache, without the need to duplicate cache values on each machine. To take advantage of this feature, include all server addresses in CACHE_BACKEND, separated by semicolons.

Memcached一个明显的优势就是可以在多台服务器之间共享缓存,(传说中的分布式??),而memcached对待这些想对待一台机器一样.

In this example, the cache is shared over Memcached instances running on IP address 172.19.26.240 and 172.19.26.242, both on port 11211:
这个例子中在两台机器间共享缓存，都是在11211端口
CACHE_BACKEND = 'memcached://172.19.26.240:11211;172.19.26.242:11211/'
In the following example, the cache is shared over Memcached instances running on the IP addresses 172.19.26.240 (port 11211), 172.19.26.242 (port 11212), and 172.19.26.244 (port 11213):

CACHE_BACKEND = 'memcached://172.19.26.240:11211;172.19.26.242:11212;172.19.26.244:11213/'

A final point about Memcached is that memory-based caching has one disadvantage: Because the cached data is stored in memory, the data will be lost if your server crashes. Clearly, memory is not intended for permanent data storage, so do not rely on memory-based caching as your only data storage. Without a doubt, none of the Django caching backends should be used for permanent storage -- they are all intended to be solutions for caching, not storage -- but we point this out here because memory-based caching is particularly temporary.
关于Memcached最后一点就是基于内存的缓存有个弊端，就是缓存的数据保存在内存里，当服务器当了，数据就没有了，很明显，内存不是用来作持久化存储的，所以不要把基于内存的缓存作为唯一的数据存储，毫无疑问所有的django缓存的不应用作持久的存储，，他们是缓存的解决方案，不是存储，我们在这里指出就是因为基于内存的缓存是短暂的，

Database caching
To use a database table as your cache backend, first create a cache table in your database by running this command:

python manage.py createcachetable [cache_table_name]
...where [cache_table_name] is the name of the database table to create. (This name can be whatever you want, as long as it is a valid table name that is not already being used in your database.) This command creates a single table in your database that is in the proper format that Django·s database-cache system expects.

Once you have created that database table, set your CACHE_BACKEND setting to "db://tablename", where tablename is the name of the database table. In this example, the cache table·s name is my_cache_table:

CACHE_BACKEND = 'db://my_cache_table'
The database caching backend uses the same database as specified in your settings file. You can not use a different database backend for your cache table.

Database caching works best if you have got a fast, well-indexed database server.

DATABASE CACHING AND MULTIPLE DATABASES
If you use database caching with multiple databases, you'll also need to set up routing instructions for your database cache table. For the purposes of routing, the database cache table appears as a model named CacheEntry, in an application named django_cache. This model won't appear in the models cache, but the model details can be used for routing purposes.

For example, the following router would direct all cache read operations to cache_slave, and all write operations to cache_master. The cache table will only be synchronized onto cache_master:

class CacheRouter(object):
    """A router to control all database cache operations"""

    def db_for_read(self, model, **hints):
        "All cache read operations go to the slave"
        if model._meta.app_label in ('django_cache',):
            return 'cache_slave'
        return None

    def db_for_write(self, model, **hints):
        "All cache write operations go to master"
        if model._meta.app_label in ('django_cache',):
            return 'cache_master'
        return None

    def allow_syncdb(self, db, model):
        "Only synchronize the cache model on master"
        if model._meta.app_label in ('django_cache',):
            return db == 'cache_master'
        return None
If you don't specify routing directions for the database cache model, the cache backend will use the default database.

Of course, if you don't use the database cache backend, you don't need to worry about providing routing instructions for the database cache model.

Filesystem caching
To store cached items on a filesystem, use the "file://" cache type for CACHE_BACKEND. For example, to store cached data in /var/tmp/django_cache, use this setting:

CACHE_BACKEND = 'file:///var/tmp/django_cache'
Note that there are three forward slashes toward the beginning of that example. The first two are for file://, and the third is the first character of the directory path, /var/tmp/django_cache. If you're on Windows, put the drive letter after the file://, like this:

file://c:/foo/bar
The directory path should be absolute -- that is, it should start at the root of your filesystem. It doesn't matter whether you put a slash at the end of the setting.

Make sure the directory pointed-to by this setting exists and is readable and writable by the system user under which your Web server runs. Continuing the above example, if your server runs as the user apache, make sure the directory /var/tmp/django_cache exists and is readable and writable by the user apache.

Each cache value will be stored as a separate file whose contents are the cache data saved in a serialized ("pickled") format, using Python's pickle module. Each file's name is the cache key, escaped for safe filesystem use.

Local-memory caching
If you want the speed advantages of in-memory caching but don't have the capability of running Memcached, consider the local-memory cache backend. This cache is multi-process and thread-safe. To use it, set CACHE_BACKEND to "locmem://". For example:

CACHE_BACKEND = 'locmem://'
Note that each process will have its own private cache instance, which means no cross-process caching is possible. This obviously also means the local memory cache isn't particularly memory-efficient, so it's probably not a good choice for production environments. It's nice for development.

Dummy caching (for development)
Finally, Django comes with a "dummy" cache that doesn't actually cache -- it just implements the cache interface without doing anything.

This is useful if you have a production site that uses heavy-duty caching in various places but a development/test environment where you don't want to cache and don't want to have to change your code to special-case the latter. To activate dummy caching, set CACHE_BACKEND like so:

CACHE_BACKEND = 'dummy://'
Using a custom cache backend
New in Django 1.0: Please, see the release notes
While Django includes support for a number of cache backends out-of-the-box, sometimes you might want to use a customized cache backend. To use an external cache backend with Django, use a Python import path as the scheme portion (the part before the initial colon) of the CACHE_BACKEND URI, like so:

CACHE_BACKEND = 'path.to.backend://'
If you're building your own backend, you can use the standard cache backends as reference implementations. You'll find the code in the django/core/cache/backends/ directory of the Django source.

Note: Without a really compelling reason, such as a host that doesn't support them, you should stick to the cache backends included with Django. They've been well-tested and are easy to use.

CACHE_BACKEND arguments
Each cache backend may take arguments. They're given in query-string style on the CACHE_BACKEND setting. Valid arguments are as follows:

timeout: The default timeout, in seconds, to use for the cache. This argument defaults to 300 seconds (5 minutes).

max_entries: For the locmem, filesystem and database backends, the maximum number of entries allowed in the cache before old values are deleted. This argument defaults to 300.

cull_frequency: The fraction of entries that are culled when max_entries is reached. The actual ratio is 1/cull_frequency, so set cull_frequency=2 to cull half of the entries when max_entries is reached.

A value of 0 for cull_frequency means that the entire cache will be dumped when max_entries is reached. This makes culling much faster at the expense of more cache misses.

In this example, timeout is set to 60:

CACHE_BACKEND = "memcached://127.0.0.1:11211/?timeout=60"
In this example, timeout is 30 and max_entries is 400:

CACHE_BACKEND = "locmem://?timeout=30&amp;max_entries=400"
Invalid arguments are silently ignored, as are invalid values of known arguments.

The per-site cache
Changed in Django 1.0: (previous versions of Django only provided a single CacheMiddleware instead of the two pieces described below).
Once the cache is set up, the simplest way to use caching is to cache your entire site. You'll need to add 'django.middleware.cache.UpdateCacheMiddleware' and 'django.middleware.cache.FetchFromCacheMiddleware' to your MIDDLEWARE_CLASSES setting, as in this example:

MIDDLEWARE_CLASSES = (
    'django.middleware.cache.UpdateCacheMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.cache.FetchFromCacheMiddleware',
)
Note
No, that's not a typo: the "update" middleware must be first in the list, and the "fetch" middleware must be last. The details are a bit obscure, but see Order of MIDDLEWARE_CLASSES below if you'd like the full story.
Then, add the following required settings to your Django settings file:

CACHE_MIDDLEWARE_SECONDS -- The number of seconds each page should be cached.
CACHE_MIDDLEWARE_KEY_PREFIX -- If the cache is shared across multiple sites using the same Django installation, set this to the name of the site, or some other string that is unique to this Django instance, to prevent key collisions. Use an empty string if you don't care.
The cache middleware caches every page that doesn't have GET or POST parameters. Optionally, if the CACHE_MIDDLEWARE_ANONYMOUS_ONLY setting is True, only anonymous requests (i.e., not those made by a logged-in user) will be cached. This is a simple and effective way of disabling caching for any user-specific pages (include Django's admin interface). Note that if you use CACHE_MIDDLEWARE_ANONYMOUS_ONLY, you should make sure you've activated AuthenticationMiddleware. The cache middleware expects that a HEAD request is answered with the same response headers exactly like the corresponding GET request, in that case it could return cached GET response for HEAD request.

Additionally, the cache middleware automatically sets a few headers in each HttpResponse:

Sets the Last-Modified header to the current date/time when a fresh (uncached) version of the page is requested.
Sets the Expires header to the current date/time plus the defined CACHE_MIDDLEWARE_SECONDS.
Sets the Cache-Control header to give a max age for the page -- again, from the CACHE_MIDDLEWARE_SECONDS setting.
See Middleware for more on middleware.

New in Django 1.0: Please, see the release notes
If a view sets its own cache expiry time (i.e. it has a max-age section in its Cache-Control header) then the page will be cached until the expiry time, rather than CACHE_MIDDLEWARE_SECONDS. Using the decorators in django.views.decorators.cache you can easily set a view's expiry time (using the cache_control decorator) or disable caching for a view (using the never_cache decorator). See the using other headers section for more on these decorators.

New in Django 1.2: Please, see the release notes
If USE_I18N is set to True then the generated cache key will include the name of the active language. This allows you to easily cache multilingual sites without having to create the cache key yourself.

See Deployment of translations for more on how Django discovers the active language.

The per-view cache
A more granular way to use the caching framework is by caching the output of individual views. django.views.decorators.cache defines a cache_page decorator that will automatically cache the view's response for you. It's easy to use:

from django.views.decorators.cache import cache_page

@cache_page(60 * 15)
def my_view(request):
    ...
cache_page takes a single argument: the cache timeout, in seconds. In the above example, the result of the my_view() view will be cached for 15 minutes. (Note that we've written it as 60 * 15 for the purpose of readability. 60 * 15 will be evaluated to 900 -- that is, 15 minutes multiplied by 60 seconds per minute.)

The per-view cache, like the per-site cache, is keyed off of the URL. If multiple URLs point at the same view, each URL will be cached separately. Continuing the my_view example, if your URLconf looks like this:

urlpatterns = ('',
    (r'^foo/(\d{1,2})/$', my_view),
)
then requests to /foo/1/ and /foo/23/ will be cached separately, as you may expect. But once a particular URL (e.g., /foo/23/) has been requested, subsequent requests to that URL will use the cache.

cache_page can also take an optional keyword argument, key_prefix, which works in the same way as the CACHE_MIDDLEWARE_KEY_PREFIX setting for the middleware. It can be used like this:

@cache_page(60 * 15, key_prefix="site1")
def my_view(request):
    ...
Specifying per-view cache in the URLconf
The examples in the previous section have hard-coded the fact that the view is cached, because cache_page alters the my_view function in place. This approach couples your view to the cache system, which is not ideal for several reasons. For instance, you might want to reuse the view functions on another, cache-less site, or you might want to distribute the views to people who might want to use them without being cached. The solution to these problems is to specify the per-view cache in the URLconf rather than next to the view functions themselves.

Doing so is easy: simply wrap the view function with cache_page when you refer to it in the URLconf. Here's the old URLconf from earlier:

urlpatterns = ('',
    (r'^foo/(\d{1,2})/$', my_view),
)
Here's the same thing, with my_view wrapped in cache_page:

from django.views.decorators.cache import cache_page

urlpatterns = ('',
    (r'^foo/(\d{1,2})/$', cache_page(my_view, 60 * 15)),
)
If you take this approach, don't forget to import cache_page within your URLconf.

Template fragment caching
New in Django 1.0: Please, see the release notes
If you're after even more control, you can also cache template fragments using the cache template tag. To give your template access to this tag, put {% load cache %} near the top of your template.

The {% cache %} template tag caches the contents of the block for a given amount of time. It takes at least two arguments: the cache timeout, in seconds, and the name to give the cache fragment. For example:

{% load cache %}
{% cache 500 sidebar %}
    .. sidebar ..
{% endcache %}
Sometimes you might want to cache multiple copies of a fragment depending on some dynamic data that appears inside the fragment. For example, you might want a separate cached copy of the sidebar used in the previous example for every user of your site. Do this by passing additional arguments to the {% cache %} template tag to uniquely identify the cache fragment:

{% load cache %}
{% cache 500 sidebar request.user.username %}
    .. sidebar for logged in user ..
{% endcache %}
It's perfectly fine to specify more than one argument to identify the fragment. Simply pass as many arguments to {% cache %} as you need.

If USE_I18N is set to True the per-site middleware cache will respect the active language. For the cache template tag you could use one of the translation-specific variables available in templates to archieve the same result:

{% load i18n %}
{% load cache %}

{% get_current_language as LANGUAGE_CODE %}

{% cache 600 welcome LANGUAGE_CODE %}
    {% trans "Welcome to example.com" %}
{% endcache %}
The cache timeout can be a template variable, as long as the template variable resolves to an integer value. For example, if the template variable my_timeout is set to the value 600, then the following two examples are equivalent:

{% cache 600 sidebar %} ... {% endcache %}
{% cache my_timeout sidebar %} ... {% endcache %}
This feature is useful in avoiding repetition in templates. You can set the timeout in a variable, in one place, and just reuse that value.

The low-level cache API
Sometimes, caching an entire rendered page doesn't gain you very much and is, in fact, inconvenient overkill.

Perhaps, for instance, your site includes a view whose results depend on several expensive queries, the results of which change at different intervals. In this case, it would not be ideal to use the full-page caching that the per-site or per-view cache strategies offer, because you would not want to cache the entire result (since some of the data changes often), but you would still want to cache the results that rarely change.
或许，比如你的站点包含一个view，他的结果以来于几个昂贵的查询，这个结果依赖于查询条件的变化，这样，整个页面的缓存不是一个好的处理方法，因为我们不想缓存整个结果，，因为有些数据变化频繁，但是还是要缓存某些不怎么变化的结果

For cases like this, Django exposes a simple, low-level cache API. You can use this API to store objects in the cache with any level of granularity you like. You can cache any Python object that can be pickled safely: strings, dictionaries, lists of model objects, and so forth. (Most common Python objects can be pickled; refer to the Python documentation for more information about pickling.)

The cache module, django.core.cache, has a cache object that's automatically created from the CACHE_BACKEND setting:

&gt;&gt;&gt; from django.core.cache import cache
The basic interface is set(key, value, timeout) and get(key):

&gt;&gt;&gt; cache.set('my_key', 'hello, world!', 30)
&gt;&gt;&gt; cache.get('my_key')
'hello, world!'
The timeout argument is optional and defaults to the timeout argument in the CACHE_BACKEND setting (explained above). It's the number of seconds the value should be stored in the cache.

If the object doesn't exist in the cache, cache.get() returns None:

# Wait 30 seconds for 'my_key' to expire...

&gt;&gt;&gt; cache.get('my_key')
None
We advise against storing the literal value None in the cache, because you won't be able to distinguish between your stored None value and a cache miss signified by a return value of None.

cache.get() can take a default argument. This specifies which value to return if the object doesn't exist in the cache:

&gt;&gt;&gt; cache.get('my_key', 'has expired')
'has expired'
New in Django 1.0: Please, see the release notes
To add a key only if it doesn't already exist, use the add() method. It takes the same parameters as set(), but it will not attempt to update the cache if the key specified is already present:

&gt;&gt;&gt; cache.set('add_key', 'Initial value')
&gt;&gt;&gt; cache.add('add_key', 'New value')
&gt;&gt;&gt; cache.get('add_key')
'Initial value'
If you need to know whether add() stored a value in the cache, you can check the return value. It will return True if the value was stored, False otherwise.

There's also a get_many() interface that only hits the cache once. get_many() returns a dictionary with all the keys you asked for that actually exist in the cache (and haven't expired):

&gt;&gt;&gt; cache.set('a', 1)
&gt;&gt;&gt; cache.set('b', 2)
&gt;&gt;&gt; cache.set('c', 3)
&gt;&gt;&gt; cache.get_many(['a', 'b', 'c'])
{'a': 1, 'b': 2, 'c': 3}
New in Django 1.2: Please, see the release notes
To set multiple values more efficiently, use set_many() to pass a dictionary of key-value pairs:

&gt;&gt;&gt; cache.set_many({'a': 1, 'b': 2, 'c': 3})
&gt;&gt;&gt; cache.get_many(['a', 'b', 'c'])
{'a': 1, 'b': 2, 'c': 3}
Like cache.set(), set_many() takes an optional timeout parameter.

You can delete keys explicitly with delete(). This is an easy way of clearing the cache for a particular object:

&gt;&gt;&gt; cache.delete('a')
New in Django 1.2: Please, see the release notes
If you want to clear a bunch of keys at once, delete_many() can take a list of keys to be cleared:

&gt;&gt;&gt; cache.delete_many(['a', 'b', 'c'])
New in Django 1.2: Please, see the release notes
Finally, if you want to delete all the keys in the cache, use cache.clear(). Be careful with this; clear() will remove everything from the cache, not just the keys set by your application.

&gt;&gt;&gt; cache.clear()
New in Django 1.1: Please, see the release notes
You can also increment or decrement a key that already exists using the incr() or decr() methods, respectively. By default, the existing cache value will incremented or decremented by 1. Other increment/decrement values can be specified by providing an argument to the increment/decrement call. A ValueError will be raised if you attempt to increment or decrement a nonexistent cache key.:
##注意这个，如果需要清除缓存，或者更新某个缓存，这个可以用
&gt;&gt;&gt; cache.set('num', 1)
&gt;&gt;&gt; cache.incr('num')
2
&gt;&gt;&gt; cache.incr('num', 10)
12
&gt;&gt;&gt; cache.decr('num')
11
&gt;&gt;&gt; cache.decr('num', 5)
6
Note
incr()/decr() methods are not guaranteed to be atomic. On those backends that support atomic increment/decrement (most notably, the memcached backend), increment and decrement operations will be atomic. However, if the backend doesn't natively provide an increment/decrement operation, it will be implemented using a two-step retrieve/update.
Cache key warnings
New in Django Development version.
Memcached, the most commonly-used production cache backend, does not allow cache keys longer than 250 characters or containing whitespace or control characters, and using such keys will cause an exception. To encourage cache-portable code and minimize unpleasant surprises, the other built-in cache backends issue a warning (django.core.cache.backends.base.CacheKeyWarning) if a key is used that would cause an error on memcached.

If you are using a production backend that can accept a wider range of keys (a custom backend, or one of the non-memcached built-in backends), and want to use this wider range without warnings, you can silence CacheKeyWarning with this code in the management module of one of your INSTALLED_APPS:

import warnings

from django.core.cache import CacheKeyWarning

warnings.simplefilter("ignore", CacheKeyWarning)
If you want to instead provide custom key validation logic for one of the built-in backends, you can subclass it, override just the validate_key method, and follow the instructions for using a custom cache backend. For instance, to do this for the locmem backend, put this code in a module:

from django.core.cache.backends.locmem import CacheClass as LocMemCacheClass

class CacheClass(LocMemCacheClass):
    def validate_key(self, key):
        """Custom validation, raising exceptions or warnings as needed."""
        # ...
...and use the dotted Python path to this module as the scheme portion of your CACHE_BACKEND.

Upstream caches
So far, this document has focused on caching your own data. But another type of caching is relevant to Web development, too: caching performed by "upstream" caches. These are systems that cache pages for users even before the request reaches your Web site.

Here are a few examples of upstream caches:

Your ISP may cache certain pages, so if you requested a page from http://example.com/, your ISP would send you the page without having to access example.com directly. The maintainers of example.com have no knowledge of this caching; the ISP sits between example.com and your Web browser, handling all of the caching transparently.
Your Django Web site may sit behind a proxy cache, such as Squid Web Proxy Cache (http://www.squid-cache.org/), that caches pages for performance. In this case, each request first would be handled by the proxy, and it would be passed to your application only if needed.
Your Web browser caches pages, too. If a Web page sends out the appropriate headers, your browser will use the local cached copy for subsequent requests to that page, without even contacting the Web page again to see whether it has changed.
Upstream caching is a nice efficiency boost, but there's a danger to it: Many Web pages' contents differ based on authentication and a host of other variables, and cache systems that blindly save pages based purely on URLs could expose incorrect or sensitive data to subsequent visitors to those pages.

For example, say you operate a Web e-mail system, and the contents of the "inbox" page obviously depend on which user is logged in. If an ISP blindly cached your site, then the first user who logged in through that ISP would have his user-specific inbox page cached for subsequent visitors to the site. That's not cool.

Fortunately, HTTP provides a solution to this problem. A number of HTTP headers exist to instruct upstream caches to differ their cache contents depending on designated variables, and to tell caching mechanisms not to cache particular pages. We'll look at some of these headers in the sections that follow.

Using Vary headers
The Vary header defines which request headers a cache mechanism should take into account when building its cache key. For example, if the contents of a Web page depend on a user's language preference, the page is said to "vary on language."

By default, Django's cache system creates its cache keys using the requested path (e.g., "/stories/2005/jun/23/bank_robbed/"). This means every request to that URL will use the same cached version, regardless of user-agent differences such as cookies or language preferences. However, if this page produces different content based on some difference in request headers -- such as a cookie, or a language, or a user-agent -- you'll need to use the Vary header to tell caching mechanisms that the page output depends on those things.

To do this in Django, use the convenient vary_on_headers view decorator, like so:

from django.views.decorators.vary import vary_on_headers

@vary_on_headers('User-Agent')
def my_view(request):
    # ...
In this case, a caching mechanism (such as Django's own cache middleware) will cache a separate version of the page for each unique user-agent.

The advantage to using the vary_on_headers decorator rather than manually setting the Vary header (using something like response['Vary'] = 'user-agent') is that the decorator adds to the Vary header (which may already exist), rather than setting it from scratch and potentially overriding anything that was already in there.

You can pass multiple headers to vary_on_headers():

@vary_on_headers('User-Agent', 'Cookie')
def my_view(request):
    # ...
This tells upstream caches to vary on both, which means each combination of user-agent and cookie will get its own cache value. For example, a request with the user-agent Mozilla and the cookie value foo=bar will be considered different from a request with the user-agent Mozilla and the cookie value foo=ham.

Because varying on cookie is so common, there's a vary_on_cookie decorator. These two views are equivalent:

@vary_on_cookie
def my_view(request):
    # ...

@vary_on_headers('Cookie')
def my_view(request):
    # ...
The headers you pass to vary_on_headers are not case sensitive; "User-Agent" is the same thing as "user-agent".

You can also use a helper function, django.utils.cache.patch_vary_headers, directly. This function sets, or adds to, the Vary header. For example:

from django.utils.cache import patch_vary_headers

def my_view(request):
    # ...
    response = render_to_response('template_name', context)
    patch_vary_headers(response, ['Cookie'])
    return response
patch_vary_headers takes an HttpResponse instance as its first argument and a list/tuple of case-insensitive header names as its second argument.

For more on Vary headers, see the official Vary spec.

Controlling cache: Using other headers
Other problems with caching are the privacy of data and the question of where data should be stored in a cascade of caches.

A user usually faces two kinds of caches: his or her own browser cache (a private cache) and his or her provider's cache (a public cache). A public cache is used by multiple users and controlled by someone else. This poses problems with sensitive data--you don't want, say, your bank account number stored in a public cache. So Web applications need a way to tell caches which data is private and which is public.

The solution is to indicate a page's cache should be "private." To do this in Django, use the cache_control view decorator. Example:

from django.views.decorators.cache import cache_control

@cache_control(private=True)
def my_view(request):
    # ...
This decorator takes care of sending out the appropriate HTTP header behind the scenes.

There are a few other ways to control cache parameters. For example, HTTP allows applications to do the following:

Define the maximum time a page should be cached.
Specify whether a cache should always check for newer versions, only delivering the cached content when there are no changes. (Some caches might deliver cached content even if the server page changed, simply because the cache copy isn't yet expired.)
In Django, use the cache_control view decorator to specify these cache parameters. In this example, cache_control tells caches to revalidate the cache on every access and to store cached versions for, at most, 3,600 seconds:

from django.views.decorators.cache import cache_control

@cache_control(must_revalidate=True, max_age=3600)
def my_view(request):
    # ...
Any valid Cache-Control HTTP directive is valid in cache_control(). Here's a full list:

public=True
private=True
no_cache=True
no_transform=True
must_revalidate=True
proxy_revalidate=True
max_age=num_seconds
s_maxage=num_seconds
For explanation of Cache-Control HTTP directives, see the Cache-Control spec.

(Note that the caching middleware already sets the cache header's max-age with the value of the CACHE_MIDDLEWARE_SETTINGS setting. If you use a custom max_age in a cache_control decorator, the decorator will take precedence, and the header values will be merged correctly.)

If you want to use headers to disable caching altogether, django.views.decorators.cache.never_cache is a view decorator that adds headers to ensure the response won't be cached by browsers or other caches. Example:

from django.views.decorators.cache import never_cache

@never_cache
def myview(request):
    # ...
Other optimizations
Django comes with a few other pieces of middleware that can help optimize your site's performance:

django.middleware.http.ConditionalGetMiddleware adds support for modern browsers to conditionally GET responses based on the ETag and Last-Modified headers.
django.middleware.gzip.GZipMiddleware compresses responses for all moderns browsers, saving bandwidth and transfer time.
Order of MIDDLEWARE_CLASSES
If you use caching middleware, it's important to put each half in the right place within the MIDDLEWARE_CLASSES setting. That's because the cache middleware needs to know which headers by which to vary the cache storage. Middleware always adds something to the Vary response header when it can.

UpdateCacheMiddleware runs during the response phase, where middleware is run in reverse order, so an item at the top of the list runs last during the response phase. Thus, you need to make sure that UpdateCacheMiddleware appears before any other middleware that might add something to the Vary header. The following middleware modules do so:

SessionMiddleware adds Cookie
GZipMiddleware adds Accept-Encoding
LocaleMiddleware adds Accept-Language
FetchFromCacheMiddleware, on the other hand, runs during the request phase, where middleware is applied first-to-last, so an item at the top of the list runs first during the request phase. The FetchFromCacheMiddleware also needs to run after other middleware updates the Vary header, so FetchFromCacheMiddleware must be after any item that does so.</t>
<t tx="newlife.20101108160359.1387">import logging
from types import GeneratorType
from django.core.cache import cache as djcache
from django.db.models.query import QuerySet
from django.contrib.contenttypes.models import ContentType

class CachedQS(object):
    """ Cached Query Set to store Query set in Memcache/DjangCache
    To customize it, please rewrite 2 methods:
      - get_key()
      - get_qs()
    """
    def get(self):
        key = self.get_key()
        c = djcache.get(key)
        if not c:
            c = self.purge(key)
        return c

    def get_key(self):
        raise NotImplementedError

    def get_qs(self):
        raise NotImplementedError

    def update(self):
        key = self.get_key()
        return self.purge(key)
        
    def delete(self):
        key = self.get_key()
        djcache.delete(key)
        
    def purge(self, key):
        c = self.get_qs()
        if isinstance(c, (QuerySet, GeneratorType)):
            c = list(c)
            if len(c) &gt; 1000:
                logging.error("The length of queryset exceeds 100",
                              exec_info=True)
                c = None
        if c:
            djcache.set(key, c)
        return c
##这个是个辅助类   
class ModelCachedQS(CachedQS):
    def __init__(self, model, obj_id):
        ctype = ContentType.objects.get_for_model(model)
        self._key = 'm:%s-%s' % (ctype.id, obj_id)
        self.model = model
        self.obj_id = obj_id

    def get_key(self):
        return self._key

    def get_qs(self):
        return self.model.objects.get(id=self.obj_id)
##这行的意义在于缓存某个表的对象，可是有神马意义呢。就一个单一的对象的能有多大的效率提升？？



</t>
<t tx="newlife.20101108160359.1388">from django.core.cache import cache
from django.db import models
from django.conf import settings

DOMAIN_CACHE_PREFIX = settings.CACHE_MIDDLEWARE_KEY_PREFIX
CACHE_EXPIRE = settings.CACHE_MIDDLEWARE_SECONDS

def cache_key(model, id):
    return (“%s-%s-%s” % (DOMAIN_CACHE_PREFIX, model._meta.db_table,id)).replace(” “, “”)

class GetCacheManager(models.Manager):
    # to reload the get method. cache -&gt; db -&gt; cache
    def get(self, *args, **kwargs):
        id = repr(kwargs)
    # in mc, data are stored in {pointer_key -&gt; model_key},{model_key -&gt; model}
    # pointer_key is object.get’s method parameters.
    # pointer_key = ‘www-user-{‘username’:'qingran}’ or ‘www-user-{‘id’:1}’
    pointer_key = cache_key(self.model, id)

    # model_key is “&lt;prefix&gt;-&lt;db tablename&gt;-pk”
    # model_key = ‘www-user-1′
    model_key = cache.get(pointer_key)

    if model_key != None:
        model = cache.get(model_key)
            if model != None:
                return model

    # cache MISS, get from db.
    model = super(GetCacheManager, self).get(*args, **kwargs)

    # write data back to cache from db.
    if not model_key:
        model_key = cache_key(model, model.pk)
        cache.set(pointer_key, model_key, CACHE_EXPIRE)
        cache.set(model_key, model, CACHE_EXPIRE)
        return model


class ModelWithGetCache(models.Model):
    # to reload the save method
    def save(self, *args, **kwargs):
        # first, delete cache {model_key -&gt; model}
        model_key = cache_key(self, self.pk)
        cache.delete(model_key)
        super(ModelWithGetCache, self).save()

# to reload the delete method
def delete(self, *args, **kwargs):
    # first, delete cache {model_key -&gt; model}
    model_key = cache_key(self, self.pk)
    cache.delete(model_key)
super(ModelWithGetCache, self).delete()

===============================================

在使用的时候定义models需要改从ModelWithGetCache继承，并且指定objects = GetCacheManager()

Sample:
class User(ModelWithGetCache):
objects = GetCacheManager()
…</t>
<t tx="newlife.20101109103546.1391">ms可以写一个东西，</t>
<t tx="newlife.20101109103546.1392"></t>
<t tx="newlife.20101109103546.1393">一个监控memcached的django插件</t>
<t tx="newlife.20101109103546.1394">"""A full cache system written on top of Django's rudimentary one."""

from django.conf import settings
from django.core.cache import cache
from django.utils.encoding import smart_str
from django.utils.hashcompat import md5_constructor
from keyedcache.utils import is_string_like, is_list_or_tuple
import cPickle as pickle
import logging
import types

log = logging.getLogger('keyedcache')

CACHED_KEYS = {}
CACHE_CALLS = 0
CACHE_HITS = 0
KEY_DELIM = "::"
REQUEST_CACHE = {'enabled' : False}
try:
    CACHE_PREFIX = settings.CACHE_PREFIX
except AttributeError:
    CACHE_PREFIX = str(settings.SITE_ID)
    log.warn("No CACHE_PREFIX found in settings, using SITE_ID.  Please update your settings to add a CACHE_PREFIX")

try:
    CACHE_TIMEOUT = settings.CACHE_TIMEOUT
except AttributeError:
    CACHE_TIMEOUT = 0
    log.warn("No CACHE_TIMEOUT found in settings, so we used 0, disabling the cache system.  Please update your settings to add a CACHE_TIMEOUT and avoid this warning.")

_CACHE_ENABLED = CACHE_TIMEOUT &gt; 0

class CacheWrapper(object):
    def __init__(self, val, inprocess=False):
        self.val = val
        self.inprocess = inprocess

    def __str__(self):
        return str(self.val)

    def __repr__(self):
        return repr(self.val)

    def wrap(cls, obj):
        if isinstance(obj, cls):
            return obj
        else:
            return cls(obj)

    wrap = classmethod(wrap)

class MethodNotFinishedError(Exception):
    def __init__(self, f):
        self.func = f


class NotCachedError(Exception):
    def __init__(self, k):
        self.key = k

class CacheNotRespondingError(Exception):
    pass

def cache_delete(*keys, **kwargs):
    removed = []
    if cache_enabled():
        global CACHED_KEYS
        log.debug('cache_delete')
        children = kwargs.pop('children',False)

        if (keys or kwargs):
            key = cache_key(*keys, **kwargs)

            if CACHED_KEYS.has_key(key):
                del CACHED_KEYS[key]
                removed.append(key)

            cache.delete(key)

            if children:
                key = key + KEY_DELIM
                children = [x for x in CACHED_KEYS.keys() if x.startswith(key)]
                for k in children:
                    del CACHED_KEYS[k]
                    cache.delete(k)
                    removed.append(k)
        else:
            key = "All Keys"
            deleteneeded = _cache_flush_all()

            removed = CACHED_KEYS.keys()

            if deleteneeded:
                for k in CACHED_KEYS:
                    cache.delete(k)

            CACHED_KEYS = {}

        if removed:
            log.debug("Cache delete: %s", removed)
        else:
            log.debug("No cached objects to delete for %s", key)

    return removed


def cache_delete_function(func):
    return cache_delete(['func', func.__name__, func.__module__], children=True)

def cache_enabled():
    global _CACHE_ENABLED
    return _CACHE_ENABLED

def cache_enable(state=True):
    global _CACHE_ENABLED
    _CACHE_ENABLED=state

def _cache_flush_all():
    if is_memcached_backend():
        cache._cache.flush_all()
        return False
    return True

def cache_function(length=CACHE_TIMEOUT):
    """
    A variant of the snippet posted by Jeff Wheeler at
    http://www.djangosnippets.org/snippets/109/

    Caches a function, using the function and its arguments as the key, and the return
    value as the value saved. It passes all arguments on to the function, as
    it should.

    The decorator itself takes a length argument, which is the number of
    seconds the cache will keep the result around.

    It will put a temp value in the cache while the function is
    processing. This should not matter in most cases, but if the app is using
    threads, you won't be able to get the previous value, and will need to
    wait until the function finishes. If this is not desired behavior, you can
    remove the first two lines after the ``else``.
    """
    def decorator(func):
        def inner_func(*args, **kwargs):
            if not cache_enabled():
                value = func(*args, **kwargs)

            else:
                try:
                    value = cache_get('func', func.__name__, func.__module__, args, kwargs)

                except NotCachedError, e:
                    # This will set a temporary value while ``func`` is being
                    # processed. When using threads, this is vital, as otherwise
                    # the function can be called several times before it finishes
                    # and is put into the cache.
                    funcwrapper = CacheWrapper(".".join([func.__module__, func.__name__]), inprocess=True)
                    cache_set(e.key, value=funcwrapper, length=length, skiplog=True)
                    value = func(*args, **kwargs)
                    cache_set(e.key, value=value, length=length)

                except MethodNotFinishedError, e:
                    value = func(*args, **kwargs)

            return value
        return inner_func
    return decorator


def cache_get(*keys, **kwargs):
    if kwargs.has_key('default'):
        default_value = kwargs.pop('default')
        use_default = True
    else:
        use_default = False

    key = cache_key(keys, **kwargs)

    if not cache_enabled():
        raise NotCachedError(key)
    else:
        global CACHE_CALLS, CACHE_HITS, REQUEST_CACHE
        CACHE_CALLS += 1
        if CACHE_CALLS == 1:
            cache_require()

        obj = None
        tid = -1
        if REQUEST_CACHE['enabled']:
            tid = cache_get_request_uid()
            if tid &gt; -1:
                try:
                    obj = REQUEST_CACHE[tid][key]
                    log.debug('Got from request cache: %s', key)
                except KeyError:
                    pass

        if obj == None:
            obj = cache.get(key)

        if obj and isinstance(obj, CacheWrapper):
            CACHE_HITS += 1
            CACHED_KEYS[key] = True
            log.debug('got cached [%i/%i]: %s', CACHE_CALLS, CACHE_HITS, key)
            if obj.inprocess:
                raise MethodNotFinishedError(obj.val)

            cache_set_request(key, obj, uid=tid)

            return obj.val
        else:
            try:
                del CACHED_KEYS[key]
            except KeyError:
                pass

            if use_default:
                return default_value

            raise NotCachedError(key)


def cache_set(*keys, **kwargs):
    """Set an object into the cache."""
    if cache_enabled():
        global CACHED_KEYS, REQUEST_CACHE
        obj = kwargs.pop('value')
        length = kwargs.pop('length', CACHE_TIMEOUT)
        skiplog = kwargs.pop('skiplog', False)

        key = cache_key(keys, **kwargs)
        val = CacheWrapper.wrap(obj)
        if not skiplog:
            log.debug('setting cache: %s', key)
        cache.set(key, val, length)
        CACHED_KEYS[key] = True
        if REQUEST_CACHE['enabled']:
            cache_set_request(key, val)

def _hash_or_string(key):
    if is_string_like(key) or isinstance(key, (types.IntType, types.LongType, types.FloatType)):
        return smart_str(key)
    else:
        try:
            #if it has a PK, use it.
            return str(key._get_pk_val())
        except AttributeError:
            return md5_hash(key)

def cache_contains(*keys, **kwargs):
    key = cache_key(keys, **kwargs)
    return CACHED_KEYS.has_key(key)

def cache_key(*keys, **pairs):
    """Smart key maker, returns the object itself if a key, else a list
    delimited by ':', automatically hashing any non-scalar objects."""

    if is_string_like(keys):
        keys = [keys]

    if is_list_or_tuple(keys):
        if len(keys) == 1 and is_list_or_tuple(keys[0]):
            keys = keys[0]
    else:
        keys = [md5_hash(keys)]

    if pairs:
        keys = list(keys)
        klist = pairs.keys()
        klist.sort()
        for k in klist:
            keys.append(k)
            keys.append(pairs[k])

    key = KEY_DELIM.join([_hash_or_string(x) for x in keys])
    prefix = CACHE_PREFIX + KEY_DELIM
    if not key.startswith(prefix):
        key = prefix+key
    return key.replace(" ", ".")

def md5_hash(obj):
    pickled = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)
    return md5_constructor(pickled).hexdigest()


def is_memcached_backend():
    try:
        return cache._cache.__module__.endswith('memcache')
    except AttributeError:
        return False

def cache_require():
    """Error if keyedcache isn't running."""
    if cache_enabled():
        key = cache_key('require_cache')
        cache_set(key,value='1')
        v = cache_get(key, default = '0')
        if v != '1':
            raise CacheNotRespondingError()
        else:
            log.debug("Cache responding OK")
        return True

def cache_clear_request(uid):
    """Clears all locally cached elements with that uid"""
    global REQUEST_CACHE
    try:
        del REQUEST_CACHE[uid]
        log.debug('cleared request cache: %s', uid)
    except KeyError:
        pass

def cache_use_request_caching():
    global REQUEST_CACHE
    REQUEST_CACHE['enabled'] = True

def cache_get_request_uid():
    from threaded_multihost import threadlocals
    return threadlocals.get_thread_variable('request_uid', -1)

def cache_set_request(key, val, uid=None):
    if uid == None:
        uid = cache_get_request_uid()

    if uid&gt;-1:
        global REQUEST_CACHE
        if not uid in REQUEST_CACHE:
            REQUEST_CACHE[uid] = {key:val}
        else:
            REQUEST_CACHE[uid][key] = val
</t>
<t tx="newlife.20101109103546.1395">关于这个项目的思路：首先需要说这个东西是干神马的，，ms没有看到处理y处理memcached的东西。。
这是为毛阿。。</t>
<t tx="newlife.20101109103546.1396">import socket, StringIO

'''
@author: ahuaxuan
@date: 2008-10-22
'''

class mcstats(object):
    
    def __init__(self, address, port):
        self.address = address
        self.port = port
        
        self.s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.s.connect((self.address, self.port))
        ##伟大的socket来了，，，
    
    def __del__(self):
        self.s.close()
        
    def connect(self, command):   
        ##接受的参数是stats items \r\n     
        num = self.s.send(command)        
        totalData = ''
        while True:
            data = self.s.recv(1024)
            if len(data) &lt;= 0:
                break            
            totalData = totalData + data
            if totalData.find('END') &gt;= 0:
                break            
        return totalData
        
    def calcSlabsCount(self, data):
        f = StringIO.StringIO(data)
        tmp = None
        for a in f:            
            if a.find('END') &gt;= 0:
                break
            else:
                tmp = a
        f.close()
        if tmp != None:
            return int(tmp.split(":")[1])
        else:
            return 0
        
    def showKVpairs(self, slabCounts, command):
        for a in range(0, slabCounts):
            tmpc = command + str(a + 1) + ' 0 \r\n'
            data = self.connect(tmpc)
            f = StringIO.StringIO(data)
            for b in f:
                if b.find('ITEM') &gt;= 0:
                    arr = b.split(' ')
                    print 'key : ' + arr[1] + ' ------ value size : ' + arr[2][1:len(arr[2])]        
            f.close()
            
if __name__ == "__main__":
    mcs = mcstats("localhost", 11211)
    mcs.showKVpairs(mcs.calcSlabsCount(mcs.connect('stats items \r\n')), 'stats cachedump ')
</t>
<t tx="newlife.20101110110210.1403">def get_memcache(name,function):
    """
    this is a function which is to set the value in the memcache
    """
    value = function()
    mc.set(name,value)
    
    """
    trade off :
        where should Iput this fuction to ,to use the django`s method ? howerver ,it is too diffculf to understand....
    """
    
    用法，找出最关键的执行语句，这很难阿。。</t>
<t tx="newlife.20101110110210.1404">"""
Decorator for views that tries getting the page from the cache and
populates the cache if the page isn't in the cache yet.

The cache is keyed by the URL and some data from the headers. Additionally
there is the key prefix that is used to distinguish different cache areas
in a multi-site setup. You could use the sites.get_current().domain, for
example, as that is unique across a Django project.

Additionally, all headers from the response's Vary header will be taken into
account on caching -- just like the middleware does.
"""

try:
    from functools import wraps
except ImportError:
    from django.utils.functional import wraps  # Python 2.4 fallback.

from django.utils.decorators import decorator_from_middleware_with_args, available_attrs
from django.utils.cache import patch_cache_control, add_never_cache_headers
from django.middleware.cache import CacheMiddleware


def cache_page(*args, **kwargs):
    # We need backwards compatibility with code which spells it this way:
    #   def my_view(): pass
    #   my_view = cache_page(my_view, 123)
    # and this way:
    #   my_view = cache_page(123)(my_view)
    # and this:
    #   my_view = cache_page(my_view, 123, key_prefix="foo")
    # and this:
    #   my_view = cache_page(123, key_prefix="foo")(my_view)
    # and possibly this way (?):
    #   my_view = cache_page(123, my_view)

    # We also add some asserts to give better error messages in case people are
    # using other ways to call cache_page that no longer work.
    key_prefix = kwargs.pop('key_prefix', None)
    assert not kwargs, "The only keyword argument accepted is key_prefix"
    if len(args) &gt; 1:
        assert len(args) == 2, "cache_page accepts at most 2 arguments"
        if callable(args[0]):
            平日那天
            return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[1], key_prefix=key_prefix)(args[0])
        elif callable(args[1]):
            return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], key_prefix=key_prefix)(args[1])
        else:
            assert False, "cache_page must be passed either a single argument (timeout) or a view function and a timeout"
    else:
        return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], key_prefix=key_prefix)


def cache_control(**kwargs):
    def _cache_controller(viewfunc):
        def _cache_controlled(request, *args, **kw):
            response = viewfunc(request, *args, **kw)
            patch_cache_control(response, **kwargs)
            return response
        return wraps(viewfunc, assigned=available_attrs(viewfunc))(_cache_controlled)
    return _cache_controller


def never_cache(view_func):
    """
    Decorator that adds headers to a response so that it will
    never be cached.
    """
    def _wrapped_view_func(request, *args, **kwargs):
        response = view_func(request, *args, **kwargs)
        add_never_cache_headers(response)
        return response
    return wraps(view_func, assigned=available_attrs(view_func))(_wrapped_view_func)
</t>
<t tx="newlife.20101110110210.1405">"""
Cache middleware. If enabled, each Django-powered page will be cached based on
URL. The canonical way to enable cache middleware is to set
``UpdateCacheMiddleware`` as your first piece of middleware, and
``FetchFromCacheMiddleware`` as the last::

    MIDDLEWARE_CLASSES = [
        'django.middleware.cache.UpdateCacheMiddleware',
        ...
        'django.middleware.cache.FetchFromCacheMiddleware'
    ]

This is counter-intuitive, but correct: ``UpdateCacheMiddleware`` needs to run
last during the response phase, which processes middleware bottom-up;
``FetchFromCacheMiddleware`` needs to run last during the request phase, which
processes middleware top-down.

The single-class ``CacheMiddleware`` can be used for some simple sites.
However, if any other piece of middleware needs to affect the cache key, you'll
need to use the two-part ``UpdateCacheMiddleware`` and
``FetchFromCacheMiddleware``. This'll most often happen when you're using
Django's ``LocaleMiddleware``.

More details about how the caching works:

* Only parameter-less GET or HEAD-requests with status code 200 are cached.

* The number of seconds each page is stored for is set by the "max-age" section
  of the response's "Cache-Control" header, falling back to the
  CACHE_MIDDLEWARE_SECONDS setting if the section was not found.

* If CACHE_MIDDLEWARE_ANONYMOUS_ONLY is set to True, only anonymous requests
  (i.e., those not made by a logged-in user) will be cached. This is a simple
  and effective way of avoiding the caching of the Django admin (and any other
  user-specific content).

* This middleware expects that a HEAD request is answered with a response
  exactly like the corresponding GET request.

* When a hit occurs, a shallow copy of the original response object is returned
  from process_request.

* Pages will be cached based on the contents of the request headers listed in
  the response's "Vary" header.

* This middleware also sets ETag, Last-Modified, Expires and Cache-Control
  headers on the response object.

"""

from django.conf import settings
from django.core.cache import cache
from django.utils.cache import get_cache_key, learn_cache_key, patch_response_headers, get_max_age

class UpdateCacheMiddleware(object):
    """
    Response-phase cache middleware that updates the cache if the response is
    cacheable.

    Must be used as part of the two-part update/fetch cache middleware.
    UpdateCacheMiddleware must be the first piece of middleware in
    MIDDLEWARE_CLASSES so that it'll get called last during the response phase.
    """
    def __init__(self):
        self.cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS
        self.key_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX
        self.cache_anonymous_only = getattr(settings, 'CACHE_MIDDLEWARE_ANONYMOUS_ONLY', False)

    def process_response(self, request, response):
        """Sets the cache, if needed."""
        if not hasattr(request, '_cache_update_cache') or not request._cache_update_cache:
            # We don't need to update the cache, just return.
            return response
        if request.method != 'GET':
            # This is a stronger requirement than above. It is needed
            # because of interactions between this middleware and the
            # HTTPMiddleware, which throws the body of a HEAD-request
            # away before this middleware gets a chance to cache it.
            return response
        if not response.status_code == 200:
            return response
        # Try to get the timeout from the "max-age" section of the "Cache-
        # Control" header before reverting to using the default cache_timeout
        # length.
        timeout = get_max_age(response)
        if timeout == None:
            timeout = self.cache_timeout
        elif timeout == 0:
            # max-age was set to 0, don't bother caching.
            return response
        patch_response_headers(response, timeout)
        if timeout:
            cache_key = learn_cache_key(request, response, timeout, self.key_prefix)
            cache.set(cache_key, response, timeout)
        return response

class FetchFromCacheMiddleware(object):
    """
    Request-phase cache middleware that fetches a page from the cache.

    Must be used as part of the two-part update/fetch cache middleware.
    FetchFromCacheMiddleware must be the last piece of middleware in
    MIDDLEWARE_CLASSES so that it'll get called last during the request phase.
    """
    def __init__(self):
        self.cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS
        self.key_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX
        self.cache_anonymous_only = getattr(settings, 'CACHE_MIDDLEWARE_ANONYMOUS_ONLY', False)

    def process_request(self, request):
        """
        Checks whether the page is already cached and returns the cached
        version if available.
        """
        if self.cache_anonymous_only:
            assert hasattr(request, 'user'), "The Django cache middleware with CACHE_MIDDLEWARE_ANONYMOUS_ONLY=True requires authentication middleware to be installed. Edit your MIDDLEWARE_CLASSES setting to insert 'django.contrib.auth.middleware.AuthenticationMiddleware' before the CacheMiddleware."

        if not request.method in ('GET', 'HEAD') or request.GET:
            request._cache_update_cache = False
            return None # Don't bother checking the cache.

        if self.cache_anonymous_only and request.user.is_authenticated():
            request._cache_update_cache = False
            return None # Don't cache requests from authenticated users.

        cache_key = get_cache_key(request, self.key_prefix)
        if cache_key is None:
            request._cache_update_cache = True
            return None # No cache information available, need to rebuild.

        response = cache.get(cache_key, None)
        if response is None:
            request._cache_update_cache = True
            return None # No cache information available, need to rebuild.

        request._cache_update_cache = False
        return response

class CacheMiddleware(UpdateCacheMiddleware, FetchFromCacheMiddleware):
    """
    Cache middleware that provides basic behavior for many simple sites.

    Also used as the hook point for the cache decorator, which is generated
    using the decorator-from-middleware utility.
    """
    def __init__(self, cache_timeout=None, key_prefix=None, cache_anonymous_only=None):
        self.cache_timeout = cache_timeout
        if cache_timeout is None:
            self.cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS
        self.key_prefix = key_prefix
        if key_prefix is None:
            self.key_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX
        if cache_anonymous_only is None:
            self.cache_anonymous_only = getattr(settings, 'CACHE_MIDDLEWARE_ANONYMOUS_ONLY', False)
        else:
            self.cache_anonymous_only = cache_anonymous_only
</t>
<t tx="newlife.20101110110210.1406">"Functions that help with dynamically creating decorators for views."

import types
try:
    from functools import wraps, update_wrapper, WRAPPER_ASSIGNMENTS
except ImportError:
    from django.utils.functional import wraps, update_wrapper, WRAPPER_ASSIGNMENTS  # Python 2.4 fallback.


def method_decorator(decorator):
    """
    Converts a function decorator into a method decorator
    """
    def _dec(func):
        def _wrapper(self, *args, **kwargs):
            def bound_func(*args2, **kwargs2):
                return func(self, *args2, **kwargs2)
            # bound_func has the signature that 'decorator' expects i.e.  no
            # 'self' argument, but it is a closure over self so it can call
            # 'func' correctly.
            return decorator(bound_func)(*args, **kwargs)
        return wraps(func)(_wrapper)
    update_wrapper(_dec, decorator)
    # Change the name to aid debugging.
    _dec.__name__ = 'method_decorator(%s)' % decorator.__name__
    return _dec


def decorator_from_middleware_with_args(middleware_class):
    """
    Like decorator_from_middleware, but returns a function
    that accepts the arguments to be passed to the middleware_class.
    Use like::

         cache_page = decorator_from_middleware_with_args(CacheMiddleware)
         # ...

         @cache_page(3600)
         def my_view(request):
             # ...
    """
    return make_middleware_decorator(middleware_class)


def decorator_from_middleware(middleware_class):
    """
    Given a middleware class (not an instance), returns a view decorator. This
    lets you use middleware functionality on a per-view basis. The middleware
    is created with no params passed.
    """
    return make_middleware_decorator(middleware_class)()


def available_attrs(fn):
    """
    Return the list of functools-wrappable attributes on a callable.
    This is required as a workaround for http://bugs.python.org/issue3445.
    """
    return tuple(a for a in WRAPPER_ASSIGNMENTS if hasattr(fn, a))


def make_middleware_decorator(middleware_class):
    def _make_decorator(*m_args, **m_kwargs):
        middleware = middleware_class(*m_args, **m_kwargs)
        def _decorator(view_func):
            def _wrapped_view(request, *args, **kwargs):
                if hasattr(middleware, 'process_request'):
                    result = middleware.process_request(request)
                    if result is not None:
                        return result
                if hasattr(middleware, 'process_view'):
                    result = middleware.process_view(request, view_func, args, kwargs)
                    if result is not None:
                        return result
                try:
                    response = view_func(request, *args, **kwargs)
                except Exception, e:
                    if hasattr(middleware, 'process_exception'):
                        result = middleware.process_exception(request, e)
                        if result is not None:
                            return result
                    raise
                if hasattr(middleware, 'process_response'):
                    result = middleware.process_response(request, response)
                    if result is not None:
                        return result
                return response
            return wraps(view_func, assigned=available_attrs(view_func))(_wrapped_view)
        return _decorator
    return _make_decorator
</t>
<t tx="newlife.20101110110210.1407">这个东西写的太他们恶心了，，，</t>
<t tx="newlife.20101110110210.1408">Low-level cache decorators for Django
http://fi.am/entry/low-level-cache-decorators-for-django/

from django.core.cache import cache

def cache_set(key, value):
    cache.set(key, value)
    return value
##这个是辅助类，就是操作缓存
def cached_property(func):
    def cached_func(self):
        key = 'cached_property_%s_%s_%s' % \
            (self.__class__.__name__, func.__name__, self.pk)
        return cache.get(key) or cache_set(key, func(self))

    return property(cached_func)
"""
Just one drawback: if None (or anything evaluating to False, like '' or []) is a valid return value for the property, you are making two unneeded trips to the cache every time you return None. In those cases, you are probably better adding the cache logic to the function itself or using this other approach, which provides object level in-memory caching:
就是有个弊端：
"""
def stored_property(func):
    key = '_cached_%s' % func.__name__
    def stored_func(self):
        if not hasattr(self, key):
            setattr(self, key, func(self))
        return getattr(self, key)

     return property(stored_func)
"""
Alternativelly, sometimes you can also use a helper function and apply the cache decorator it, leaving the corner cases returning None for the public function:
    
"""
@cached_property
def _foo(self):
     return some_expensive_operation(self)

@stored_property
def foo(self): # This detects the cases when foo is None and skips the cache
    if self.some_condition:
         return None
     return self._foo
    
"""
For methods receiving parameters, you can also use this technique with the following decorator:   
对于处理有参数的方法，我们可以用下面的方法
"""
def cached_method(func):
    def cached_func(self, *args, **kwargs): 
        key = 'cached_method_%s_%s_%s_%s_%s' % \
            (self.__class__.__name__, func.__name__, self.pk, hash(args),
             hash(kwargs.iteritems()))
        return cache.get(key) or cache_set(key, func(self, *args, **kwargs))

    return cached_func</t>
<t tx="newlife.20101112095312.1415">有参数的decorator：
    由于有参数的装饰器函数在调用的时候只会使用应用是的参数，而不会接收被修饰的函数作为参数，所以必须返回一个装饰器函数，由它对被修饰的函数进行封装。

简单的说，就是真正的装饰器只接收一个函数作为参数，所谓有参数的装饰器只不过是装饰器的装饰器</t>
<t tx="newlife.20101112095312.1416">def myview(request):
    @request.cache.cache('my_search_func', expire=3600)
    def get_results(search_param):
        # do something to retrieve data
        data = get_data(search_param)
        return data
    results = get_results('demo')
    return Response(results)
    


#! /usr/bin/env python
#coding=utf-8
from django.core.cache import cache
from django.utils.functional import wraps


def cache_set(key, value):
    cache.set(key, value)
    return value


def cached_key_method(func):
    """
    this decorator is used for the situatuion where user need to assign the key in memcached    
    """
    def cached_func(*args, **kwargs): 
        key = 'cached_method_%s'%(args[0])
        return cache.get(key) or cache_set(key, func(*args, **kwargs))

    return cached_func

def cached(key_input):
    def _cached(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            key = 'cached_method_%s'%(key_input)
            return cache.get(key) or cache_set(key, func(*args, **kwargs))            
        return wrapper
    return _cached
一下午都在折腾这个，有个问题就是接收的参数是key_input，开始的时候写key是不可以的，
这是个神奇的问题，，，
</t>
<t tx="newlife.20101117145351.1419">def curry(_curried_func, *args, **kwargs):
    def _curried(*moreargs, **morekwargs):
        return _curried_func(*(args+moreargs), **dict(kwargs, **morekwargs))
    return _curried

### Begin from Python 2.5 functools.py ########################################

# Summary of changes made to the Python 2.5 code below:
#   * swapped ``partial`` for ``curry`` to maintain backwards-compatibility
#     in Django.

# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007 Python Software Foundation.
# All Rights Reserved.

###############################################################################

# update_wrapper() and wraps() are tools to help write
# wrapper functions that can handle naive introspection

WRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__doc__')
WRAPPER_UPDATES = ('__dict__',)
def update_wrapper(wrapper,
                   wrapped,
                   assigned = WRAPPER_ASSIGNMENTS,
                   updated = WRAPPER_UPDATES):
    """Update a wrapper function to look like the wrapped function

       wrapper is the function to be updated
       wrapped is the original function
       assigned is a tuple naming the attributes assigned directly
       from the wrapped function to the wrapper function (defaults to
       functools.WRAPPER_ASSIGNMENTS)
       updated is a tuple naming the attributes off the wrapper that
       are updated with the corresponding attribute from the wrapped
       function (defaults to functools.WRAPPER_UPDATES)
    """
    for attr in assigned:
        setattr(wrapper, attr, getattr(wrapped, attr))
    for attr in updated:
        getattr(wrapper, attr).update(getattr(wrapped, attr))
    # Return the wrapper so this can be used as a decorator via curry()
    return wrapper

def wraps(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to wraps() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying curry() to
       update_wrapper().
    """
    return curry(update_wrapper, wrapped=wrapped,
                 assigned=assigned, updated=updated)</t>
<t tx="newlife.20101117145351.1420">When you use a decorator, you are replacing one function with another. In other words, if you have a decorator
当你用一个装饰器的时候，你在用一个函数替代另一个。换句话说，你有这样一个装饰器，

def logged(func):
    def with_logging(*args, **kwargs):
        print func.__name__ + " was called"
        return func(*args, **kwargs)
    return with_logging

then when you say
然后这样用
@logged
def f(x):
   """does some math"""
   return x + x * x

it is exactly the same as saying
这是不是就是和下面这个一样，
def f(x):
    """does some math"""
    return x + x * x
f = logged(f)
and your function f is replaced with the function with_logging. Unfortunately, this means that if you then say
并且函数f被with_logging代替，不幸的是，打印f的名字得出的是新函数的名字。
print f.__name__

it will print with_logging because that is the name of your new function. 

In fact, if you look at the docstring for f, it will be blank because with_logging has no docstring, and so the docstring you wrote will not be there anymore. Also, if you look at the pydoc result for that function, it will not be listed as taking one argument x; instead it will be listed as taking *args and **kwargs because that is what with_logging takes.
实际上，观察函数f的docstring，没有东西，因为withlogging 没有docstring，然后你写的docstring也就不在这里了，

If using a decorator always meant losing this information about a function, it would be a serious problem. That is why we have functools.wraps. This takes a function used in a decorator and adds the functionality of copying over the function name, docstring, arguments list, etc. And since wraps is itself a decorator, the following code does the correct thing:

如果使用装饰器，这就意味着我们要失去这个函数的信息，这是个严重的问题，这就是为啥有functools.wraps, 这个函数接收一个在装饰器里用的函数作为参数，并且把函数的名字阿，docstring阿，参数列表阿，并且wraps本身也是一个
装饰器，剩下的代码作真正的事情，
from functools import wraps
def logged(func):
    @wraps(func)
    def with_logging(*args, **kwargs):
        print func.__name__ + " was called"
        return func(*args, **kwargs)
    return with_logging

@logged
def f(x):
   """does some math"""
   return x + x * x

print f.__name__  # prints 'f'
print f.__doc__   # prints 'does some math'</t>
<t tx="newlife.20101117145351.1421">import warnings
from functools import wraps

def deprecated(func):
    @wraps(func)
    def new_func(*args, **kwargs):
        warnings.warn("This function is old", DeprecationWarning,
                      2)
        return func(*args, **kwargs)
    return new_func

@deprecated
def i_am_old(foo, bar):
    """ This is a super-old docstring """
    return foo, bar

def change_doc(docstring):
    def decorator(func):
        func.__doc__ = docstring
        return func
    return decorator

#@change_doc("My super-new docstring")
def foo(bar):
    """ My old docstring """
    return bar

foo = change_doc("My super-new docstring")(foo)

if __name__ == '__main__':
    #print foo.__doc__
    print i_am_old(42, "Answer")</t>
</tnodes>
</leo_file>
